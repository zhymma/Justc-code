2023-07-07 16:06:27,555 - get train data loader...
2023-07-07 16:06:27,733 - get dev data loader...
2023-07-07 16:06:27,775 - define model...
2023-07-07 16:06:52,133 - define optimizer...
2023-07-07 16:06:52,139 - ===========================train setting parameters=========================
2023-07-07 16:06:52,141 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 16:06:52,141 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 16:06:52,141 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 16:06:52,142 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,142 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,143 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,144 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,144 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,144 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,145 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,146 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,147 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,147 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,148 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,154 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,154 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,155 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,155 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,156 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,156 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,156 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,156 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,162 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,163 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,163 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,163 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,163 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,164 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,165 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,165 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,165 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,165 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,165 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,165 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 16:06:52,165 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 16:06:52,165 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,166 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 16:06:52,167 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 16:06:52,168 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 16:06:52,168 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 16:06:52,168 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 16:06:52,168 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 16:06:52,168 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 16:06:52,168 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 16:06:52,168 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 16:06:52,168 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 16:06:52,168 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 16:06:52,169 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 16:06:52,169 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 16:06:52,169 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 16:06:52,169 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 16:06:52,169 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 16:06:52,169 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 16:06:52,169 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 16:06:52,169 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 16:06:52,169 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 16:06:52,169 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 16:06:52,169 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 16:06:52,169 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 16:06:52,169 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 16:06:52,169 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 16:06:52,169 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 16:06:52,177 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 16:06:52,177 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 16:06:52,177 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 16:06:52,177 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 16:06:52,189 - 
>>>>>>>>>>>>>>>>>>>start train......
2023-07-07 16:07:11,118 - 


2023-07-07 16:07:11,124 - epoch:0,	loss:1.93722003698349
2023-07-07 16:07:14,404 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 16:07:14,405 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 16:07:14,405 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 16:07:14,407 - save best model to ./output/test/best_model
2023-07-07 16:07:36,883 - 


2023-07-07 16:07:36,883 - epoch:1,	loss:1.2210435029119253
2023-07-07 16:07:40,041 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 16:07:40,041 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 16:07:40,041 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 16:07:51,377 - 


2023-07-07 16:07:51,377 - epoch:2,	loss:0.9560508746653795
2023-07-07 16:07:54,620 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 16:07:54,620 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 16:07:54,626 - wrong_be_tree_count:432	wrong_total:433	 wrong be tree ACC: 0.9976905311778291
2023-07-07 16:08:06,111 - 


2023-07-07 16:08:06,111 - epoch:3,	loss:0.8729819413274527
2023-07-07 16:08:09,392 - right_count:7	total:433	 Answer ACC: 0.016166281755196306
2023-07-07 16:08:09,392 - right_codes_count:1	total:433	 Code ACC: 0.0023094688221709007
2023-07-07 16:08:09,392 - wrong_be_tree_count:341	wrong_total:426	 wrong be tree ACC: 0.8004694835680751
2023-07-07 16:08:09,398 - save best model to ./output/test/best_model
2023-07-07 16:08:29,303 - 


2023-07-07 16:08:29,303 - epoch:4,	loss:0.8312085755169392
2023-07-07 16:08:32,456 - right_count:27	total:433	 Answer ACC: 0.06235565819861432
2023-07-07 16:08:32,456 - right_codes_count:10	total:433	 Code ACC: 0.023094688221709007
2023-07-07 16:08:32,456 - wrong_be_tree_count:267	wrong_total:406	 wrong be tree ACC: 0.6576354679802956
2023-07-07 16:08:32,458 - save best model to ./output/test/best_model
2023-07-07 16:08:50,649 - 


2023-07-07 16:08:50,649 - epoch:5,	loss:0.7774491822347045
2023-07-07 16:08:53,941 - right_count:90	total:433	 Answer ACC: 0.20785219399538107
2023-07-07 16:08:53,941 - right_codes_count:62	total:433	 Code ACC: 0.14318706697459585
2023-07-07 16:08:53,941 - wrong_be_tree_count:190	wrong_total:343	 wrong be tree ACC: 0.5539358600583091
2023-07-07 16:08:53,943 - save best model to ./output/test/best_model
2023-07-07 16:09:10,686 - 


2023-07-07 16:09:10,686 - epoch:6,	loss:0.7170977080240846
2023-07-07 16:09:13,880 - right_count:103	total:433	 Answer ACC: 0.23787528868360278
2023-07-07 16:09:13,880 - right_codes_count:76	total:433	 Code ACC: 0.17551963048498845
2023-07-07 16:09:13,880 - wrong_be_tree_count:109	wrong_total:330	 wrong be tree ACC: 0.3303030303030303
2023-07-07 16:09:13,883 - save best model to ./output/test/best_model
2023-07-07 16:09:31,971 - 


2023-07-07 16:09:31,972 - epoch:7,	loss:0.6597112752497196
2023-07-07 16:09:35,247 - right_count:146	total:433	 Answer ACC: 0.3371824480369515
2023-07-07 16:09:35,247 - right_codes_count:115	total:433	 Code ACC: 0.26558891454965355
2023-07-07 16:09:35,247 - wrong_be_tree_count:72	wrong_total:287	 wrong be tree ACC: 0.2508710801393728
2023-07-07 16:09:35,249 - save best model to ./output/test/best_model
2023-07-07 16:09:55,246 - 


2023-07-07 16:09:55,246 - epoch:8,	loss:0.6083915559574962
2023-07-07 16:09:58,574 - right_count:163	total:433	 Answer ACC: 0.37644341801385683
2023-07-07 16:09:58,574 - right_codes_count:134	total:433	 Code ACC: 0.3094688221709007
2023-07-07 16:09:58,574 - wrong_be_tree_count:51	wrong_total:270	 wrong be tree ACC: 0.18888888888888888
2023-07-07 16:09:58,577 - save best model to ./output/test/best_model
2023-07-07 16:10:19,710 - 


2023-07-07 16:10:19,710 - epoch:9,	loss:0.556087389588356
2023-07-07 16:10:22,787 - right_count:189	total:433	 Answer ACC: 0.43648960739030024
2023-07-07 16:10:22,787 - right_codes_count:154	total:433	 Code ACC: 0.3556581986143187
2023-07-07 16:10:22,787 - wrong_be_tree_count:38	wrong_total:244	 wrong be tree ACC: 0.1557377049180328
2023-07-07 16:10:22,789 - save best model to ./output/test/best_model
2023-07-07 16:10:44,037 - 


2023-07-07 16:10:44,037 - epoch:10,	loss:0.5088585601188242
2023-07-07 16:10:47,298 - right_count:213	total:433	 Answer ACC: 0.49191685912240185
2023-07-07 16:10:47,298 - right_codes_count:179	total:433	 Code ACC: 0.4133949191685912
2023-07-07 16:10:47,298 - wrong_be_tree_count:58	wrong_total:220	 wrong be tree ACC: 0.2636363636363636
2023-07-07 16:10:47,300 - save best model to ./output/test/best_model
2023-07-07 16:11:08,252 - 


2023-07-07 16:11:08,252 - epoch:11,	loss:0.46613616961985826
2023-07-07 16:11:11,555 - right_count:223	total:433	 Answer ACC: 0.5150115473441108
2023-07-07 16:11:11,555 - right_codes_count:193	total:433	 Code ACC: 0.4457274826789838
2023-07-07 16:11:11,555 - wrong_be_tree_count:69	wrong_total:210	 wrong be tree ACC: 0.32857142857142857
2023-07-07 16:11:11,557 - save best model to ./output/test/best_model
2023-07-07 16:11:37,959 - 


2023-07-07 16:11:37,969 - epoch:12,	loss:0.4201028570532799
2023-07-07 16:11:41,160 - right_count:246	total:433	 Answer ACC: 0.5681293302540416
2023-07-07 16:11:41,160 - right_codes_count:218	total:433	 Code ACC: 0.5034642032332564
2023-07-07 16:11:41,160 - wrong_be_tree_count:83	wrong_total:187	 wrong be tree ACC: 0.44385026737967914
2023-07-07 16:11:41,162 - save best model to ./output/test/best_model
2023-07-07 16:12:14,424 - 


2023-07-07 16:12:14,424 - epoch:13,	loss:0.38029321702197194
2023-07-07 16:12:17,611 - right_count:260	total:433	 Answer ACC: 0.6004618937644342
2023-07-07 16:12:17,612 - right_codes_count:233	total:433	 Code ACC: 0.5381062355658198
2023-07-07 16:12:17,612 - wrong_be_tree_count:81	wrong_total:173	 wrong be tree ACC: 0.4682080924855491
2023-07-07 16:12:17,623 - save best model to ./output/test/best_model
2023-07-07 16:12:46,375 - 


2023-07-07 16:12:46,375 - epoch:14,	loss:0.34414549893699586
2023-07-07 16:12:49,828 - right_count:283	total:433	 Answer ACC: 0.6535796766743649
2023-07-07 16:12:49,828 - right_codes_count:261	total:433	 Code ACC: 0.6027713625866051
2023-07-07 16:12:49,828 - wrong_be_tree_count:69	wrong_total:150	 wrong be tree ACC: 0.46
2023-07-07 16:12:49,830 - save best model to ./output/test/best_model
2023-07-07 16:13:20,378 - 


2023-07-07 16:13:20,378 - epoch:15,	loss:0.3213688733521849
2023-07-07 16:13:24,363 - right_count:288	total:433	 Answer ACC: 0.6651270207852193
2023-07-07 16:13:24,364 - right_codes_count:265	total:433	 Code ACC: 0.6120092378752887
2023-07-07 16:13:24,364 - wrong_be_tree_count:57	wrong_total:145	 wrong be tree ACC: 0.3931034482758621
2023-07-07 16:13:24,373 - save best model to ./output/test/best_model
2023-07-07 16:13:43,824 - 


2023-07-07 16:13:43,824 - epoch:16,	loss:0.30410903971642256
2023-07-07 16:13:47,202 - right_count:301	total:433	 Answer ACC: 0.6951501154734411
2023-07-07 16:13:47,203 - right_codes_count:278	total:433	 Code ACC: 0.6420323325635104
2023-07-07 16:13:47,203 - wrong_be_tree_count:49	wrong_total:132	 wrong be tree ACC: 0.3712121212121212
2023-07-07 16:13:47,205 - save best model to ./output/test/best_model
2023-07-07 16:14:05,766 - 


2023-07-07 16:14:05,766 - epoch:17,	loss:0.2821542979218066
2023-07-07 16:14:09,120 - right_count:303	total:433	 Answer ACC: 0.6997690531177829
2023-07-07 16:14:09,121 - right_codes_count:279	total:433	 Code ACC: 0.6443418013856813
2023-07-07 16:14:09,121 - wrong_be_tree_count:51	wrong_total:130	 wrong be tree ACC: 0.3923076923076923
2023-07-07 16:14:09,123 - save best model to ./output/test/best_model
2023-07-07 16:14:29,172 - 


2023-07-07 16:14:29,172 - epoch:18,	loss:0.27043105009943247
2023-07-07 16:14:32,464 - right_count:302	total:433	 Answer ACC: 0.6974595842956121
2023-07-07 16:14:32,465 - right_codes_count:284	total:433	 Code ACC: 0.6558891454965358
2023-07-07 16:14:32,465 - wrong_be_tree_count:55	wrong_total:131	 wrong be tree ACC: 0.4198473282442748
2023-07-07 16:14:44,212 - 


2023-07-07 16:14:44,212 - epoch:19,	loss:0.24519148492254317
2023-07-07 16:14:47,457 - right_count:307	total:433	 Answer ACC: 0.7090069284064665
2023-07-07 16:14:47,458 - right_codes_count:286	total:433	 Code ACC: 0.6605080831408776
2023-07-07 16:14:47,458 - wrong_be_tree_count:55	wrong_total:126	 wrong be tree ACC: 0.4365079365079365
2023-07-07 16:14:47,467 - save best model to ./output/test/best_model
2023-07-07 16:15:08,501 - 


2023-07-07 16:15:08,501 - epoch:20,	loss:0.2314369457308203
2023-07-07 16:15:11,716 - right_count:314	total:433	 Answer ACC: 0.7251732101616628
2023-07-07 16:15:11,716 - right_codes_count:297	total:433	 Code ACC: 0.6859122401847575
2023-07-07 16:15:11,716 - wrong_be_tree_count:44	wrong_total:119	 wrong be tree ACC: 0.3697478991596639
2023-07-07 16:15:11,720 - save best model to ./output/test/best_model
2023-07-07 16:15:30,572 - 


2023-07-07 16:15:30,573 - epoch:21,	loss:0.21982875559478998
2023-07-07 16:15:33,721 - right_count:316	total:433	 Answer ACC: 0.7297921478060047
2023-07-07 16:15:33,721 - right_codes_count:298	total:433	 Code ACC: 0.6882217090069284
2023-07-07 16:15:33,721 - wrong_be_tree_count:42	wrong_total:117	 wrong be tree ACC: 0.358974358974359
2023-07-07 16:15:33,729 - save best model to ./output/test/best_model
2023-07-07 16:15:55,901 - 


2023-07-07 16:15:55,901 - epoch:22,	loss:0.20526675297878683
2023-07-07 16:15:59,092 - right_count:317	total:433	 Answer ACC: 0.7321016166281755
2023-07-07 16:15:59,092 - right_codes_count:299	total:433	 Code ACC: 0.6905311778290993
2023-07-07 16:15:59,092 - wrong_be_tree_count:43	wrong_total:116	 wrong be tree ACC: 0.3706896551724138
2023-07-07 16:15:59,104 - save best model to ./output/test/best_model
2023-07-07 16:16:23,272 - 


2023-07-07 16:16:23,272 - epoch:23,	loss:0.2013801058055833
2023-07-07 16:16:26,634 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 16:16:26,634 - right_codes_count:301	total:433	 Code ACC: 0.6951501154734411
2023-07-07 16:16:26,634 - wrong_be_tree_count:41	wrong_total:111	 wrong be tree ACC: 0.36936936936936937
2023-07-07 16:16:26,637 - save best model to ./output/test/best_model
2023-07-07 16:16:52,067 - 


2023-07-07 16:16:52,068 - epoch:24,	loss:0.19289343687705696
2023-07-07 16:16:55,330 - right_count:319	total:433	 Answer ACC: 0.7367205542725174
2023-07-07 16:16:55,331 - right_codes_count:300	total:433	 Code ACC: 0.6928406466512702
2023-07-07 16:16:55,331 - wrong_be_tree_count:40	wrong_total:114	 wrong be tree ACC: 0.3508771929824561
2023-07-07 16:17:11,246 - 


2023-07-07 16:17:11,246 - epoch:25,	loss:0.18114392855204642
2023-07-07 16:17:14,436 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 16:17:14,436 - right_codes_count:300	total:433	 Code ACC: 0.6928406466512702
2023-07-07 16:17:14,436 - wrong_be_tree_count:41	wrong_total:112	 wrong be tree ACC: 0.36607142857142855
2023-07-07 16:17:29,661 - 


2023-07-07 16:17:29,661 - epoch:26,	loss:0.17790582356974483
2023-07-07 16:17:32,848 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:17:32,848 - right_codes_count:308	total:433	 Code ACC: 0.7113163972286374
2023-07-07 16:17:32,848 - wrong_be_tree_count:37	wrong_total:105	 wrong be tree ACC: 0.3523809523809524
2023-07-07 16:17:32,850 - save best model to ./output/test/best_model
2023-07-07 16:17:56,239 - 


2023-07-07 16:17:56,239 - epoch:27,	loss:0.176194301340729
2023-07-07 16:17:59,449 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 16:17:59,449 - right_codes_count:306	total:433	 Code ACC: 0.7066974595842956
2023-07-07 16:17:59,449 - wrong_be_tree_count:34	wrong_total:112	 wrong be tree ACC: 0.30357142857142855
2023-07-07 16:18:11,763 - 


2023-07-07 16:18:11,763 - epoch:28,	loss:0.16434376826509833
2023-07-07 16:18:15,358 - right_count:324	total:433	 Answer ACC: 0.7482678983833718
2023-07-07 16:18:15,358 - right_codes_count:305	total:433	 Code ACC: 0.7043879907621247
2023-07-07 16:18:15,358 - wrong_be_tree_count:40	wrong_total:109	 wrong be tree ACC: 0.3669724770642202
2023-07-07 16:18:28,891 - 


2023-07-07 16:18:28,891 - epoch:29,	loss:0.16439226875081658
2023-07-07 16:18:32,104 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 16:18:32,104 - right_codes_count:307	total:433	 Code ACC: 0.7090069284064665
2023-07-07 16:18:32,104 - wrong_be_tree_count:37	wrong_total:112	 wrong be tree ACC: 0.33035714285714285
2023-07-07 16:18:47,269 - 


2023-07-07 16:18:47,269 - epoch:30,	loss:0.162734056240879
2023-07-07 16:18:51,328 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 16:18:51,328 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 16:18:51,328 - wrong_be_tree_count:38	wrong_total:108	 wrong be tree ACC: 0.35185185185185186
2023-07-07 16:19:04,152 - 


2023-07-07 16:19:04,152 - epoch:31,	loss:0.15178991411812603
2023-07-07 16:19:07,364 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 16:19:07,364 - right_codes_count:304	total:433	 Code ACC: 0.7020785219399538
2023-07-07 16:19:07,364 - wrong_be_tree_count:34	wrong_total:111	 wrong be tree ACC: 0.3063063063063063
2023-07-07 16:19:20,618 - 


2023-07-07 16:19:20,618 - epoch:32,	loss:0.14738995872903615
2023-07-07 16:19:23,748 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 16:19:23,748 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 16:19:23,748 - wrong_be_tree_count:37	wrong_total:106	 wrong be tree ACC: 0.3490566037735849
2023-07-07 16:19:37,457 - 


2023-07-07 16:19:37,457 - epoch:33,	loss:0.14179722894914448
2023-07-07 16:19:40,834 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:19:40,834 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 16:19:40,834 - wrong_be_tree_count:33	wrong_total:102	 wrong be tree ACC: 0.3235294117647059
2023-07-07 16:19:40,836 - save best model to ./output/test/best_model
2023-07-07 16:20:00,403 - 


2023-07-07 16:20:00,403 - epoch:34,	loss:0.14049719332251698
2023-07-07 16:20:03,830 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:20:03,830 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 16:20:03,830 - wrong_be_tree_count:32	wrong_total:104	 wrong be tree ACC: 0.3076923076923077
2023-07-07 16:20:17,639 - 


2023-07-07 16:20:17,639 - epoch:35,	loss:0.14038351422641426
2023-07-07 16:20:20,906 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 16:20:20,906 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:20:20,906 - wrong_be_tree_count:35	wrong_total:108	 wrong be tree ACC: 0.32407407407407407
2023-07-07 16:20:32,499 - 


2023-07-07 16:20:32,499 - epoch:36,	loss:0.13953421404585242
2023-07-07 16:20:35,781 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:20:35,781 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:20:35,781 - wrong_be_tree_count:36	wrong_total:102	 wrong be tree ACC: 0.35294117647058826
2023-07-07 16:20:46,931 - 


2023-07-07 16:20:46,931 - epoch:37,	loss:0.13298780441982672
2023-07-07 16:20:50,191 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:20:50,191 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 16:20:50,191 - wrong_be_tree_count:35	wrong_total:105	 wrong be tree ACC: 0.3333333333333333
2023-07-07 16:21:03,187 - 


2023-07-07 16:21:03,187 - epoch:38,	loss:0.1304018198279664
2023-07-07 16:21:06,459 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:21:06,459 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:21:06,459 - wrong_be_tree_count:38	wrong_total:100	 wrong be tree ACC: 0.38
2023-07-07 16:21:06,464 - save best model to ./output/test/best_model
2023-07-07 16:21:24,810 - 


2023-07-07 16:21:24,811 - epoch:39,	loss:0.1278692380292341
2023-07-07 16:21:28,105 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:21:28,105 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:21:28,105 - wrong_be_tree_count:45	wrong_total:102	 wrong be tree ACC: 0.4411764705882353
2023-07-07 16:21:39,887 - 


2023-07-07 16:21:39,887 - epoch:40,	loss:0.12971162586472929
2023-07-07 16:21:43,060 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:21:43,060 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:21:43,060 - wrong_be_tree_count:39	wrong_total:101	 wrong be tree ACC: 0.38613861386138615
2023-07-07 16:21:54,631 - 


2023-07-07 16:21:54,631 - epoch:41,	loss:0.12092199845938012
2023-07-07 16:21:57,903 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:21:57,904 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:21:57,904 - wrong_be_tree_count:39	wrong_total:102	 wrong be tree ACC: 0.38235294117647056
2023-07-07 16:22:09,359 - 


2023-07-07 16:22:09,359 - epoch:42,	loss:0.11873060470679775
2023-07-07 16:22:12,504 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:22:12,504 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 16:22:12,505 - wrong_be_tree_count:42	wrong_total:104	 wrong be tree ACC: 0.40384615384615385
2023-07-07 16:22:23,747 - 


2023-07-07 16:22:23,747 - epoch:43,	loss:0.11642347014276311
2023-07-07 16:22:26,909 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:22:26,910 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:22:26,910 - wrong_be_tree_count:39	wrong_total:103	 wrong be tree ACC: 0.3786407766990291
2023-07-07 16:22:38,260 - 


2023-07-07 16:22:38,260 - epoch:44,	loss:0.11505602404940873
2023-07-07 16:22:41,522 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:22:41,523 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:22:41,523 - wrong_be_tree_count:44	wrong_total:102	 wrong be tree ACC: 0.43137254901960786
2023-07-07 16:22:52,972 - 


2023-07-07 16:22:52,972 - epoch:45,	loss:0.11240657250164077
2023-07-07 16:22:56,256 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:22:56,256 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:22:56,256 - wrong_be_tree_count:43	wrong_total:103	 wrong be tree ACC: 0.4174757281553398
2023-07-07 16:23:07,451 - 


2023-07-07 16:23:07,451 - epoch:46,	loss:0.10778309870511293
2023-07-07 16:23:10,624 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:23:10,625 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:23:10,625 - wrong_be_tree_count:44	wrong_total:101	 wrong be tree ACC: 0.43564356435643564
2023-07-07 16:23:22,007 - 


2023-07-07 16:23:22,007 - epoch:47,	loss:0.11299526563379914
2023-07-07 16:23:25,192 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:23:25,192 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:23:25,192 - wrong_be_tree_count:37	wrong_total:102	 wrong be tree ACC: 0.3627450980392157
2023-07-07 16:23:36,656 - 


2023-07-07 16:23:36,656 - epoch:48,	loss:0.10573645553085953
2023-07-07 16:23:39,852 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:23:39,852 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:23:39,853 - wrong_be_tree_count:38	wrong_total:98	 wrong be tree ACC: 0.3877551020408163
2023-07-07 16:23:39,854 - save best model to ./output/test/best_model
2023-07-07 16:23:57,555 - 


2023-07-07 16:23:57,555 - epoch:49,	loss:0.10527157428441569
2023-07-07 16:24:00,741 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:24:00,741 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:24:00,741 - wrong_be_tree_count:39	wrong_total:101	 wrong be tree ACC: 0.38613861386138615
2023-07-07 16:24:12,173 - 


2023-07-07 16:24:12,173 - epoch:50,	loss:0.10204894994967617
2023-07-07 16:24:15,416 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:24:15,417 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:24:15,417 - wrong_be_tree_count:39	wrong_total:103	 wrong be tree ACC: 0.3786407766990291
2023-07-07 16:24:28,100 - 


2023-07-07 16:24:28,101 - epoch:51,	loss:0.10086207068525255
2023-07-07 16:24:31,318 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:24:31,318 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:24:31,318 - wrong_be_tree_count:37	wrong_total:100	 wrong be tree ACC: 0.37
2023-07-07 16:24:42,613 - 


2023-07-07 16:24:42,613 - epoch:52,	loss:0.10004372498951852
2023-07-07 16:24:45,866 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 16:24:45,866 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:24:45,866 - wrong_be_tree_count:37	wrong_total:99	 wrong be tree ACC: 0.37373737373737376
2023-07-07 16:24:57,119 - 


2023-07-07 16:24:57,119 - epoch:53,	loss:0.09826713474467397
2023-07-07 16:25:00,374 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:25:00,374 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:25:00,374 - wrong_be_tree_count:38	wrong_total:104	 wrong be tree ACC: 0.36538461538461536
2023-07-07 16:25:11,697 - 


2023-07-07 16:25:11,697 - epoch:54,	loss:0.09561943728476763
2023-07-07 16:25:15,066 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:25:15,066 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:25:15,066 - wrong_be_tree_count:34	wrong_total:105	 wrong be tree ACC: 0.3238095238095238
2023-07-07 16:25:27,263 - 


2023-07-07 16:25:27,264 - epoch:55,	loss:0.09396973467664793
2023-07-07 16:25:30,858 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:25:30,858 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 16:25:30,858 - wrong_be_tree_count:39	wrong_total:104	 wrong be tree ACC: 0.375
2023-07-07 16:25:43,170 - 


2023-07-07 16:25:43,170 - epoch:56,	loss:0.09501565562095493
2023-07-07 16:25:46,354 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:25:46,354 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:25:46,354 - wrong_be_tree_count:37	wrong_total:104	 wrong be tree ACC: 0.3557692307692308
2023-07-07 16:25:58,141 - 


2023-07-07 16:25:58,141 - epoch:57,	loss:0.09122404101071879
2023-07-07 16:26:01,386 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 16:26:01,386 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:26:01,386 - wrong_be_tree_count:41	wrong_total:108	 wrong be tree ACC: 0.37962962962962965
2023-07-07 16:26:13,435 - 


2023-07-07 16:26:13,435 - epoch:58,	loss:0.09139801329001784
2023-07-07 16:26:16,932 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:26:16,932 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:26:16,932 - wrong_be_tree_count:41	wrong_total:103	 wrong be tree ACC: 0.39805825242718446
2023-07-07 16:26:29,289 - 


2023-07-07 16:26:29,289 - epoch:59,	loss:0.0899558705277741
2023-07-07 16:26:32,584 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:26:32,584 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:26:32,584 - wrong_be_tree_count:40	wrong_total:102	 wrong be tree ACC: 0.39215686274509803
2023-07-07 16:26:44,163 - 


2023-07-07 16:26:44,163 - epoch:60,	loss:0.09115804132306948
2023-07-07 16:26:47,362 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:26:47,362 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:26:47,362 - wrong_be_tree_count:35	wrong_total:101	 wrong be tree ACC: 0.3465346534653465
2023-07-07 16:26:59,084 - 


2023-07-07 16:26:59,084 - epoch:61,	loss:0.08812963054515421
2023-07-07 16:27:02,336 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:27:02,336 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 16:27:02,336 - wrong_be_tree_count:38	wrong_total:105	 wrong be tree ACC: 0.3619047619047619
2023-07-07 16:27:15,603 - 


2023-07-07 16:27:15,603 - epoch:62,	loss:0.08649761084234342
2023-07-07 16:27:18,839 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:27:18,839 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:27:18,840 - wrong_be_tree_count:40	wrong_total:104	 wrong be tree ACC: 0.38461538461538464
2023-07-07 16:27:32,028 - 


2023-07-07 16:27:32,029 - epoch:63,	loss:0.08316599042154849
2023-07-07 16:27:35,262 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:27:35,263 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:27:35,263 - wrong_be_tree_count:37	wrong_total:102	 wrong be tree ACC: 0.3627450980392157
2023-07-07 16:27:47,005 - 


2023-07-07 16:27:47,005 - epoch:64,	loss:0.08288775372784585
2023-07-07 16:27:50,074 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:27:50,074 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:27:50,074 - wrong_be_tree_count:31	wrong_total:100	 wrong be tree ACC: 0.31
2023-07-07 16:28:02,091 - 


2023-07-07 16:28:02,092 - epoch:65,	loss:0.08264766121283174
2023-07-07 16:28:05,490 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:28:05,490 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:28:05,490 - wrong_be_tree_count:37	wrong_total:102	 wrong be tree ACC: 0.3627450980392157
2023-07-07 16:28:17,087 - 


2023-07-07 16:28:17,087 - epoch:66,	loss:0.08210896578384563
2023-07-07 16:28:20,183 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:28:20,183 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:28:20,183 - wrong_be_tree_count:42	wrong_total:102	 wrong be tree ACC: 0.4117647058823529
2023-07-07 16:28:32,088 - 


2023-07-07 16:28:32,088 - epoch:67,	loss:0.08033686081762426
2023-07-07 16:28:35,377 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:28:35,378 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:28:35,378 - wrong_be_tree_count:42	wrong_total:104	 wrong be tree ACC: 0.40384615384615385
2023-07-07 16:28:47,094 - 


2023-07-07 16:28:47,095 - epoch:68,	loss:0.0780323474900797
2023-07-07 16:28:50,343 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:28:50,343 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 16:28:50,343 - wrong_be_tree_count:40	wrong_total:105	 wrong be tree ACC: 0.38095238095238093
2023-07-07 16:29:01,980 - 


2023-07-07 16:29:01,980 - epoch:69,	loss:0.07787053525680676
2023-07-07 16:29:05,329 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 16:29:05,330 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:29:05,330 - wrong_be_tree_count:43	wrong_total:106	 wrong be tree ACC: 0.4056603773584906
2023-07-07 16:29:16,862 - 


2023-07-07 16:29:16,862 - epoch:70,	loss:0.07635372978984378
2023-07-07 16:29:20,689 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:29:20,689 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:29:20,689 - wrong_be_tree_count:41	wrong_total:101	 wrong be tree ACC: 0.40594059405940597
2023-07-07 16:29:31,726 - 


2023-07-07 16:29:31,727 - epoch:71,	loss:0.07412000023759902
2023-07-07 16:29:35,565 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:29:35,565 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:29:35,568 - wrong_be_tree_count:34	wrong_total:103	 wrong be tree ACC: 0.3300970873786408
2023-07-07 16:29:46,433 - 


2023-07-07 16:29:46,433 - epoch:72,	loss:0.07503280614037067
2023-07-07 16:29:50,340 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:29:50,340 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:29:50,346 - wrong_be_tree_count:37	wrong_total:101	 wrong be tree ACC: 0.36633663366336633
2023-07-07 16:30:01,508 - 


2023-07-07 16:30:01,508 - epoch:73,	loss:0.07298400255967863
2023-07-07 16:30:05,483 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 16:30:05,483 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 16:30:05,498 - wrong_be_tree_count:39	wrong_total:106	 wrong be tree ACC: 0.36792452830188677
2023-07-07 16:30:16,534 - 


2023-07-07 16:30:16,535 - epoch:74,	loss:0.073134391248459
2023-07-07 16:30:20,079 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:30:20,079 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:30:20,080 - wrong_be_tree_count:40	wrong_total:104	 wrong be tree ACC: 0.38461538461538464
2023-07-07 16:30:31,842 - 


2023-07-07 16:30:31,842 - epoch:75,	loss:0.06949928915128112
2023-07-07 16:30:35,765 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 16:30:35,765 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:30:35,765 - wrong_be_tree_count:41	wrong_total:104	 wrong be tree ACC: 0.3942307692307692
2023-07-07 16:30:46,557 - 


2023-07-07 16:30:46,557 - epoch:76,	loss:0.0684566488780547
2023-07-07 16:30:50,518 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:30:50,518 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:30:50,518 - wrong_be_tree_count:40	wrong_total:101	 wrong be tree ACC: 0.39603960396039606
2023-07-07 16:31:01,579 - 


2023-07-07 16:31:01,579 - epoch:77,	loss:0.06795953717664815
2023-07-07 16:31:05,798 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 16:31:05,798 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:31:05,798 - wrong_be_tree_count:41	wrong_total:106	 wrong be tree ACC: 0.3867924528301887
2023-07-07 16:31:16,819 - 


2023-07-07 16:31:16,819 - epoch:78,	loss:0.06658283676370047
2023-07-07 16:31:19,979 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:31:19,979 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:31:19,979 - wrong_be_tree_count:39	wrong_total:102	 wrong be tree ACC: 0.38235294117647056
2023-07-07 16:31:31,873 - 


2023-07-07 16:31:31,873 - epoch:79,	loss:0.06787640516995452
2023-07-07 16:31:35,012 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:31:35,012 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:31:35,012 - wrong_be_tree_count:37	wrong_total:103	 wrong be tree ACC: 0.3592233009708738
2023-07-07 16:31:46,804 - 


2023-07-07 16:31:46,804 - epoch:80,	loss:0.06512763776117936
2023-07-07 16:31:50,083 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:31:50,083 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 16:31:50,083 - wrong_be_tree_count:37	wrong_total:105	 wrong be tree ACC: 0.3523809523809524
2023-07-07 16:32:02,065 - 


2023-07-07 16:32:02,066 - epoch:81,	loss:0.06665801504277624
2023-07-07 16:32:05,453 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 16:32:05,453 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 16:32:05,453 - wrong_be_tree_count:39	wrong_total:108	 wrong be tree ACC: 0.3611111111111111
2023-07-07 16:32:16,943 - 


2023-07-07 16:32:16,944 - epoch:82,	loss:0.0657203207956627
2023-07-07 16:32:20,119 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 16:32:20,119 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:32:20,119 - wrong_be_tree_count:41	wrong_total:107	 wrong be tree ACC: 0.38317757009345793
2023-07-07 16:32:31,625 - 


2023-07-07 16:32:31,625 - epoch:83,	loss:0.06880073726642877
2023-07-07 16:32:34,791 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:32:34,791 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:32:34,791 - wrong_be_tree_count:40	wrong_total:103	 wrong be tree ACC: 0.3883495145631068
2023-07-07 16:32:46,235 - 


2023-07-07 16:32:46,235 - epoch:84,	loss:0.062409307778580114
2023-07-07 16:32:49,639 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:32:49,639 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:32:49,639 - wrong_be_tree_count:34	wrong_total:102	 wrong be tree ACC: 0.3333333333333333
2023-07-07 16:33:00,673 - 


2023-07-07 16:33:00,674 - epoch:85,	loss:0.06199650021153502
2023-07-07 16:33:03,992 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 16:33:03,992 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 16:33:03,992 - wrong_be_tree_count:41	wrong_total:105	 wrong be tree ACC: 0.3904761904761905
2023-07-07 16:33:15,124 - 


2023-07-07 16:33:15,124 - epoch:86,	loss:0.06294505437836051
2023-07-07 16:33:18,333 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:33:18,333 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:33:18,333 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:33:30,202 - 


2023-07-07 16:33:30,202 - epoch:87,	loss:0.062278938363306224
2023-07-07 16:33:33,576 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:33:33,576 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 16:33:33,576 - wrong_be_tree_count:43	wrong_total:103	 wrong be tree ACC: 0.4174757281553398
2023-07-07 16:33:44,571 - 


2023-07-07 16:33:44,572 - epoch:88,	loss:0.062139447953086346
2023-07-07 16:33:47,762 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:33:47,762 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:33:47,762 - wrong_be_tree_count:38	wrong_total:102	 wrong be tree ACC: 0.37254901960784315
2023-07-07 16:33:59,095 - 


2023-07-07 16:33:59,095 - epoch:89,	loss:0.06156477838521823
2023-07-07 16:34:02,644 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 16:34:02,645 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 16:34:02,645 - wrong_be_tree_count:37	wrong_total:99	 wrong be tree ACC: 0.37373737373737376
2023-07-07 16:34:18,146 - 


2023-07-07 16:34:18,146 - epoch:90,	loss:0.060565645864699036
2023-07-07 16:34:21,721 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:34:21,721 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:34:21,722 - wrong_be_tree_count:42	wrong_total:100	 wrong be tree ACC: 0.42
2023-07-07 16:34:36,788 - 


2023-07-07 16:34:36,788 - epoch:91,	loss:0.06030749890487641
2023-07-07 16:34:40,018 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:34:40,018 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 16:34:40,018 - wrong_be_tree_count:41	wrong_total:98	 wrong be tree ACC: 0.41836734693877553
2023-07-07 16:34:51,870 - 


2023-07-07 16:34:51,870 - epoch:92,	loss:0.05978675413643941
2023-07-07 16:34:55,140 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:34:55,140 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:34:55,140 - wrong_be_tree_count:42	wrong_total:101	 wrong be tree ACC: 0.4158415841584158
2023-07-07 16:35:06,705 - 


2023-07-07 16:35:06,705 - epoch:93,	loss:0.060246667824685574
2023-07-07 16:35:10,182 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 16:35:10,182 - right_codes_count:322	total:433	 Code ACC: 0.74364896073903
2023-07-07 16:35:10,182 - wrong_be_tree_count:37	wrong_total:97	 wrong be tree ACC: 0.38144329896907214
2023-07-07 16:35:10,184 - save best model to ./output/test/best_model
2023-07-07 16:35:26,738 - 


2023-07-07 16:35:26,738 - epoch:94,	loss:0.058826392763876356
2023-07-07 16:35:29,978 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:35:29,978 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:35:29,978 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:35:43,187 - 


2023-07-07 16:35:43,187 - epoch:95,	loss:0.059780508949188516
2023-07-07 16:35:46,523 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 16:35:46,523 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 16:35:46,530 - wrong_be_tree_count:40	wrong_total:97	 wrong be tree ACC: 0.41237113402061853
2023-07-07 16:35:58,194 - 


2023-07-07 16:35:58,194 - epoch:96,	loss:0.05865449205157347
2023-07-07 16:36:01,830 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:36:01,831 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:36:01,831 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 16:36:12,850 - 


2023-07-07 16:36:12,851 - epoch:97,	loss:0.05785741392173804
2023-07-07 16:36:16,125 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:36:16,125 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:36:16,125 - wrong_be_tree_count:42	wrong_total:98	 wrong be tree ACC: 0.42857142857142855
2023-07-07 16:36:26,932 - 


2023-07-07 16:36:26,932 - epoch:98,	loss:0.05823084953590296
2023-07-07 16:36:30,265 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:36:30,265 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:36:30,266 - wrong_be_tree_count:43	wrong_total:101	 wrong be tree ACC: 0.42574257425742573
2023-07-07 16:36:41,835 - 


2023-07-07 16:36:41,835 - epoch:99,	loss:0.057291851699119434
2023-07-07 16:36:45,199 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:36:45,199 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:36:45,199 - wrong_be_tree_count:41	wrong_total:101	 wrong be tree ACC: 0.40594059405940597
2023-07-07 16:36:57,371 - 


2023-07-07 16:36:57,371 - epoch:100,	loss:0.057266796473413706
2023-07-07 16:37:00,675 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 16:37:00,675 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 16:37:00,675 - wrong_be_tree_count:44	wrong_total:103	 wrong be tree ACC: 0.42718446601941745
2023-07-07 16:37:13,989 - 


2023-07-07 16:37:13,989 - epoch:101,	loss:0.05661530769430101
2023-07-07 16:37:17,433 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:37:17,433 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:37:17,433 - wrong_be_tree_count:42	wrong_total:100	 wrong be tree ACC: 0.42
2023-07-07 16:37:32,411 - 


2023-07-07 16:37:32,411 - epoch:102,	loss:0.05898605435504578
2023-07-07 16:37:35,767 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:37:35,767 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 16:37:35,767 - wrong_be_tree_count:44	wrong_total:98	 wrong be tree ACC: 0.4489795918367347
2023-07-07 16:37:57,366 - 


2023-07-07 16:37:57,366 - epoch:103,	loss:0.057357063808012754
2023-07-07 16:38:00,661 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 16:38:00,661 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 16:38:00,661 - wrong_be_tree_count:42	wrong_total:99	 wrong be tree ACC: 0.42424242424242425
2023-07-07 16:38:20,853 - 


2023-07-07 16:38:20,853 - epoch:104,	loss:0.060553938790690154
2023-07-07 16:38:24,019 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:38:24,020 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:38:24,020 - wrong_be_tree_count:45	wrong_total:100	 wrong be tree ACC: 0.45
2023-07-07 16:38:43,188 - 


2023-07-07 16:38:43,188 - epoch:105,	loss:0.05678787166834809
2023-07-07 16:38:46,324 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:38:46,324 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:38:46,324 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 16:39:07,013 - 


2023-07-07 16:39:07,013 - epoch:106,	loss:0.056482033571228385
2023-07-07 16:39:10,246 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:39:10,247 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:39:10,247 - wrong_be_tree_count:41	wrong_total:101	 wrong be tree ACC: 0.40594059405940597
2023-07-07 16:39:28,075 - 


2023-07-07 16:39:28,075 - epoch:107,	loss:0.05696161088417284
2023-07-07 16:39:31,322 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:39:31,322 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:39:31,322 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:39:45,498 - 


2023-07-07 16:39:45,498 - epoch:108,	loss:0.05564448190853
2023-07-07 16:39:48,752 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:39:48,752 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:39:48,752 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:40:01,999 - 


2023-07-07 16:40:01,999 - epoch:109,	loss:0.05551907225162722
2023-07-07 16:40:05,478 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 16:40:05,478 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 16:40:05,478 - wrong_be_tree_count:40	wrong_total:96	 wrong be tree ACC: 0.4166666666666667
2023-07-07 16:40:05,481 - save best model to ./output/test/best_model
2023-07-07 16:40:24,376 - 


2023-07-07 16:40:24,377 - epoch:110,	loss:0.05507177338586189
2023-07-07 16:40:27,904 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 16:40:27,904 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 16:40:27,904 - wrong_be_tree_count:40	wrong_total:97	 wrong be tree ACC: 0.41237113402061853
2023-07-07 16:40:39,919 - 


2023-07-07 16:40:39,919 - epoch:111,	loss:0.05480067506141495
2023-07-07 16:40:43,436 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:40:43,436 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:40:43,436 - wrong_be_tree_count:42	wrong_total:101	 wrong be tree ACC: 0.4158415841584158
2023-07-07 16:40:54,901 - 


2023-07-07 16:40:54,901 - epoch:112,	loss:0.05879915569676086
2023-07-07 16:40:58,222 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 16:40:58,223 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 16:40:58,223 - wrong_be_tree_count:43	wrong_total:102	 wrong be tree ACC: 0.4215686274509804
2023-07-07 16:41:10,774 - 


2023-07-07 16:41:10,774 - epoch:113,	loss:0.05509884451748803
2023-07-07 16:41:14,501 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:41:14,502 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 16:41:14,502 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 16:41:26,727 - 


2023-07-07 16:41:26,727 - epoch:114,	loss:0.05415878845087718
2023-07-07 16:41:30,073 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:41:30,074 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:41:30,074 - wrong_be_tree_count:42	wrong_total:100	 wrong be tree ACC: 0.42
2023-07-07 16:41:42,553 - 


2023-07-07 16:41:42,553 - epoch:115,	loss:0.05516197715769522
2023-07-07 16:41:45,709 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:41:45,710 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 16:41:45,710 - wrong_be_tree_count:40	wrong_total:98	 wrong be tree ACC: 0.40816326530612246
2023-07-07 16:41:57,780 - 


2023-07-07 16:41:57,780 - epoch:116,	loss:0.054641924070892856
2023-07-07 16:42:01,001 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:42:01,001 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 16:42:01,001 - wrong_be_tree_count:40	wrong_total:98	 wrong be tree ACC: 0.40816326530612246
2023-07-07 16:42:13,201 - 


2023-07-07 16:42:13,201 - epoch:117,	loss:0.057273949088994414
2023-07-07 16:42:16,366 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 16:42:16,366 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 16:42:16,366 - wrong_be_tree_count:40	wrong_total:98	 wrong be tree ACC: 0.40816326530612246
2023-07-07 16:42:26,596 - 


2023-07-07 16:42:26,596 - epoch:118,	loss:0.05438351398333907
2023-07-07 16:42:29,691 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:42:29,691 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:42:29,691 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 16:42:39,799 - 


2023-07-07 16:42:39,799 - epoch:119,	loss:0.05445343552855775
2023-07-07 16:42:43,279 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:42:43,279 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:42:43,279 - wrong_be_tree_count:41	wrong_total:101	 wrong be tree ACC: 0.40594059405940597
2023-07-07 16:42:53,569 - 


2023-07-07 16:42:53,569 - epoch:120,	loss:0.055429710482712835
2023-07-07 16:42:56,812 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:42:56,812 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:42:56,812 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 16:43:07,151 - 


2023-07-07 16:43:07,152 - epoch:121,	loss:0.05525841671624221
2023-07-07 16:43:10,352 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:43:10,353 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:43:10,353 - wrong_be_tree_count:40	wrong_total:101	 wrong be tree ACC: 0.39603960396039606
2023-07-07 16:43:20,698 - 


2023-07-07 16:43:20,698 - epoch:122,	loss:0.05632359909941442
2023-07-07 16:43:23,833 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:43:23,833 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:43:23,833 - wrong_be_tree_count:40	wrong_total:101	 wrong be tree ACC: 0.39603960396039606
2023-07-07 16:43:34,402 - 


2023-07-07 16:43:34,402 - epoch:123,	loss:0.05411119322525337
2023-07-07 16:43:37,718 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:43:37,718 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:43:37,718 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:43:48,501 - 


2023-07-07 16:43:48,501 - epoch:124,	loss:0.054728925897507
2023-07-07 16:43:51,588 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:43:51,588 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:43:51,588 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:44:02,624 - 


2023-07-07 16:44:02,624 - epoch:125,	loss:0.05599562174757011
2023-07-07 16:44:05,677 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:44:05,677 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:44:05,677 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:44:16,494 - 


2023-07-07 16:44:16,495 - epoch:126,	loss:0.05480119519052096
2023-07-07 16:44:19,746 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 16:44:19,746 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 16:44:19,746 - wrong_be_tree_count:40	wrong_total:101	 wrong be tree ACC: 0.39603960396039606
2023-07-07 16:44:31,396 - 


2023-07-07 16:44:31,396 - epoch:127,	loss:0.05487448288477026
2023-07-07 16:44:34,648 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:44:34,648 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:44:34,649 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:44:45,853 - 


2023-07-07 16:44:45,853 - epoch:128,	loss:0.055013567267451435
2023-07-07 16:44:49,172 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:44:49,172 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:44:49,172 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 16:44:59,614 - 


2023-07-07 16:44:59,614 - epoch:129,	loss:0.056160307954996824
2023-07-07 16:45:02,811 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 16:45:02,811 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 16:45:02,811 - wrong_be_tree_count:40	wrong_total:100	 wrong be tree ACC: 0.4
2023-07-07 17:44:31,053 - get train data loader...
2023-07-07 17:44:31,145 - get dev data loader...
2023-07-07 17:44:31,205 - define model...
2023-07-07 17:44:36,972 - define optimizer...
2023-07-07 17:44:36,973 - ===========================train setting parameters=========================
2023-07-07 17:44:36,974 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 17:44:36,974 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 17:44:36,974 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 17:44:36,974 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,974 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,974 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,974 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,975 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,977 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,977 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,977 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,977 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,977 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,977 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,977 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,977 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,979 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,980 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,981 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,982 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,982 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,983 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,984 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,984 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,984 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,984 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,984 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,984 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,984 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,984 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,984 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,985 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,986 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,986 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,987 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,988 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,989 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,989 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,990 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,991 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,991 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,991 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,991 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,992 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,992 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,992 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,992 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,993 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,993 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,994 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,995 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,995 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,995 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,996 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,996 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,996 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,996 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,996 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,996 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,996 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:36,997 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:36,998 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:36,998 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,998 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:36,998 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:36,998 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 17:44:36,998 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 17:44:36,999 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 17:44:37,000 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 17:44:37,001 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 17:44:37,001 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 17:44:37,001 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 17:44:37,001 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 17:44:37,001 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 17:44:37,001 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 17:44:37,002 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 17:44:37,002 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 17:44:37,002 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 17:44:37,002 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 17:44:37,002 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 17:44:37,002 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 17:44:37,003 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 17:44:37,003 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 17:44:37,003 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 17:44:37,003 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 17:44:37,003 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 17:44:37,003 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 17:44:37,003 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 17:44:37,003 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 17:44:37,003 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 17:44:37,003 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 17:44:37,003 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 17:44:37,004 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 17:44:37,004 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 17:44:37,005 - 
>>>>>>>>>>>>>>>>>>>start train......
2023-07-07 17:44:42,706 - 


2023-07-07 17:44:42,706 - epoch:0,	loss:27.720148980617523
2023-07-07 17:44:45,887 - right_count:1	total:433	 Answer ACC: 0.0023094688221709007
2023-07-07 17:44:45,887 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 17:44:45,887 - wrong_be_tree_count:414	wrong_total:432	 wrong be tree ACC: 0.9583333333333334
2023-07-07 17:44:45,889 - save best model to ./output/test/best_model
2023-07-07 17:45:01,119 - 


2023-07-07 17:45:01,119 - epoch:1,	loss:5.365400407463312
2023-07-07 17:45:04,244 - right_count:3	total:433	 Answer ACC: 0.006928406466512702
2023-07-07 17:45:04,244 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 17:45:04,244 - wrong_be_tree_count:427	wrong_total:430	 wrong be tree ACC: 0.9930232558139535
2023-07-07 17:45:04,246 - save best model to ./output/test/best_model
2023-07-07 17:45:19,816 - 


2023-07-07 17:45:19,817 - epoch:2,	loss:1.7383414842188358
2023-07-07 17:45:22,927 - right_count:1	total:433	 Answer ACC: 0.0023094688221709007
2023-07-07 17:45:22,928 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 17:45:22,928 - wrong_be_tree_count:430	wrong_total:432	 wrong be tree ACC: 0.9953703703703703
2023-07-07 17:45:35,847 - 


2023-07-07 17:45:35,848 - epoch:3,	loss:1.2701822016388178
2023-07-07 17:45:39,012 - right_count:9	total:433	 Answer ACC: 0.020785219399538105
2023-07-07 17:45:39,012 - right_codes_count:1	total:433	 Code ACC: 0.0023094688221709007
2023-07-07 17:45:39,012 - wrong_be_tree_count:324	wrong_total:424	 wrong be tree ACC: 0.7641509433962265
2023-07-07 17:45:39,015 - save best model to ./output/test/best_model
2023-07-07 17:45:54,252 - 


2023-07-07 17:45:54,252 - epoch:4,	loss:1.1207787543535233
2023-07-07 17:45:57,480 - right_count:12	total:433	 Answer ACC: 0.02771362586605081
2023-07-07 17:45:57,480 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 17:45:57,480 - wrong_be_tree_count:304	wrong_total:421	 wrong be tree ACC: 0.7220902612826603
2023-07-07 17:45:57,483 - save best model to ./output/test/best_model
2023-07-07 17:46:13,062 - 


2023-07-07 17:46:13,062 - epoch:5,	loss:1.0266541261225939
2023-07-07 17:46:16,235 - right_count:28	total:433	 Answer ACC: 0.06466512702078522
2023-07-07 17:46:16,235 - right_codes_count:11	total:433	 Code ACC: 0.025404157043879907
2023-07-07 17:46:16,235 - wrong_be_tree_count:208	wrong_total:405	 wrong be tree ACC: 0.5135802469135803
2023-07-07 17:46:16,237 - save best model to ./output/test/best_model
2023-07-07 17:46:31,667 - 


2023-07-07 17:46:31,667 - epoch:6,	loss:0.9723202455788851
2023-07-07 17:46:34,853 - right_count:33	total:433	 Answer ACC: 0.07621247113163972
2023-07-07 17:46:34,853 - right_codes_count:16	total:433	 Code ACC: 0.03695150115473441
2023-07-07 17:46:34,853 - wrong_be_tree_count:244	wrong_total:400	 wrong be tree ACC: 0.61
2023-07-07 17:46:34,855 - save best model to ./output/test/best_model
2023-07-07 17:46:50,051 - 


2023-07-07 17:46:50,051 - epoch:7,	loss:0.9235092233866453
2023-07-07 17:46:53,303 - right_count:49	total:433	 Answer ACC: 0.11316397228637413
2023-07-07 17:46:53,303 - right_codes_count:33	total:433	 Code ACC: 0.07621247113163972
2023-07-07 17:46:53,303 - wrong_be_tree_count:255	wrong_total:384	 wrong be tree ACC: 0.6640625
2023-07-07 17:46:53,305 - save best model to ./output/test/best_model
2023-07-07 17:47:12,958 - 


2023-07-07 17:47:12,958 - epoch:8,	loss:0.882167523726821
2023-07-07 17:47:16,127 - right_count:91	total:433	 Answer ACC: 0.21016166281755197
2023-07-07 17:47:16,127 - right_codes_count:63	total:433	 Code ACC: 0.14549653579676675
2023-07-07 17:47:16,127 - wrong_be_tree_count:131	wrong_total:342	 wrong be tree ACC: 0.3830409356725146
2023-07-07 17:47:16,129 - save best model to ./output/test/best_model
2023-07-07 17:47:31,435 - 


2023-07-07 17:47:31,435 - epoch:9,	loss:0.8355464842170477
2023-07-07 17:47:34,586 - right_count:106	total:433	 Answer ACC: 0.24480369515011546
2023-07-07 17:47:34,586 - right_codes_count:76	total:433	 Code ACC: 0.17551963048498845
2023-07-07 17:47:34,586 - wrong_be_tree_count:98	wrong_total:327	 wrong be tree ACC: 0.2996941896024465
2023-07-07 17:47:34,589 - save best model to ./output/test/best_model
2023-07-07 17:47:50,127 - 


2023-07-07 17:47:50,128 - epoch:10,	loss:0.7848282782360911
2023-07-07 17:47:53,234 - right_count:122	total:433	 Answer ACC: 0.2817551963048499
2023-07-07 17:47:53,234 - right_codes_count:93	total:433	 Code ACC: 0.21478060046189376
2023-07-07 17:47:53,234 - wrong_be_tree_count:94	wrong_total:311	 wrong be tree ACC: 0.3022508038585209
2023-07-07 17:47:53,237 - save best model to ./output/test/best_model
2023-07-07 17:48:08,456 - 


2023-07-07 17:48:08,457 - epoch:11,	loss:0.7385142398998141
2023-07-07 17:48:11,654 - right_count:138	total:433	 Answer ACC: 0.3187066974595843
2023-07-07 17:48:11,654 - right_codes_count:111	total:433	 Code ACC: 0.25635103926096997
2023-07-07 17:48:11,654 - wrong_be_tree_count:60	wrong_total:295	 wrong be tree ACC: 0.2033898305084746
2023-07-07 17:48:11,656 - save best model to ./output/test/best_model
2023-07-07 17:48:26,917 - 


2023-07-07 17:48:26,917 - epoch:12,	loss:0.6937872124835849
2023-07-07 17:48:30,135 - right_count:153	total:433	 Answer ACC: 0.3533487297921478
2023-07-07 17:48:30,135 - right_codes_count:127	total:433	 Code ACC: 0.29330254041570436
2023-07-07 17:48:30,135 - wrong_be_tree_count:51	wrong_total:280	 wrong be tree ACC: 0.18214285714285713
2023-07-07 17:48:30,138 - save best model to ./output/test/best_model
2023-07-07 17:48:49,125 - 


2023-07-07 17:48:49,126 - epoch:13,	loss:0.6611379031091928
2023-07-07 17:48:52,255 - right_count:157	total:433	 Answer ACC: 0.3625866050808314
2023-07-07 17:48:52,255 - right_codes_count:128	total:433	 Code ACC: 0.2956120092378753
2023-07-07 17:48:52,255 - wrong_be_tree_count:60	wrong_total:276	 wrong be tree ACC: 0.21739130434782608
2023-07-07 17:48:52,257 - save best model to ./output/test/best_model
2023-07-07 17:49:07,266 - 


2023-07-07 17:49:07,266 - epoch:14,	loss:0.6285194428637624
2023-07-07 17:49:10,454 - right_count:169	total:433	 Answer ACC: 0.3903002309468822
2023-07-07 17:49:10,454 - right_codes_count:141	total:433	 Code ACC: 0.325635103926097
2023-07-07 17:49:10,454 - wrong_be_tree_count:46	wrong_total:264	 wrong be tree ACC: 0.17424242424242425
2023-07-07 17:49:10,456 - save best model to ./output/test/best_model
2023-07-07 17:49:25,382 - 


2023-07-07 17:49:25,382 - epoch:15,	loss:0.593063679523766
2023-07-07 17:49:28,625 - right_count:178	total:433	 Answer ACC: 0.4110854503464203
2023-07-07 17:49:28,626 - right_codes_count:144	total:433	 Code ACC: 0.3325635103926097
2023-07-07 17:49:28,626 - wrong_be_tree_count:49	wrong_total:255	 wrong be tree ACC: 0.19215686274509805
2023-07-07 17:49:28,628 - save best model to ./output/test/best_model
2023-07-07 17:49:45,069 - 


2023-07-07 17:49:45,069 - epoch:16,	loss:0.5595936896279454
2023-07-07 17:49:48,327 - right_count:188	total:433	 Answer ACC: 0.4341801385681293
2023-07-07 17:49:48,327 - right_codes_count:156	total:433	 Code ACC: 0.36027713625866054
2023-07-07 17:49:48,328 - wrong_be_tree_count:56	wrong_total:245	 wrong be tree ACC: 0.22857142857142856
2023-07-07 17:49:48,330 - save best model to ./output/test/best_model
2023-07-07 17:50:03,644 - 


2023-07-07 17:50:03,644 - epoch:17,	loss:0.5314176105894148
2023-07-07 17:50:06,786 - right_count:221	total:433	 Answer ACC: 0.5103926096997691
2023-07-07 17:50:06,786 - right_codes_count:186	total:433	 Code ACC: 0.4295612009237875
2023-07-07 17:50:06,786 - wrong_be_tree_count:63	wrong_total:212	 wrong be tree ACC: 0.2971698113207547
2023-07-07 17:50:06,789 - save best model to ./output/test/best_model
2023-07-07 17:50:27,589 - 


2023-07-07 17:50:27,589 - epoch:18,	loss:0.5094674741849303
2023-07-07 17:50:30,765 - right_count:242	total:433	 Answer ACC: 0.558891454965358
2023-07-07 17:50:30,765 - right_codes_count:199	total:433	 Code ACC: 0.45958429561200925
2023-07-07 17:50:30,765 - wrong_be_tree_count:52	wrong_total:191	 wrong be tree ACC: 0.27225130890052357
2023-07-07 17:50:30,767 - save best model to ./output/test/best_model
2023-07-07 17:50:45,876 - 


2023-07-07 17:50:45,876 - epoch:19,	loss:0.4805130837485194
2023-07-07 17:50:49,160 - right_count:243	total:433	 Answer ACC: 0.5612009237875288
2023-07-07 17:50:49,160 - right_codes_count:203	total:433	 Code ACC: 0.46882217090069284
2023-07-07 17:50:49,160 - wrong_be_tree_count:60	wrong_total:190	 wrong be tree ACC: 0.3157894736842105
2023-07-07 17:50:49,162 - save best model to ./output/test/best_model
2023-07-07 17:51:04,547 - 


2023-07-07 17:51:04,547 - epoch:20,	loss:0.4541113371960819
2023-07-07 17:51:07,682 - right_count:259	total:433	 Answer ACC: 0.5981524249422633
2023-07-07 17:51:07,682 - right_codes_count:219	total:433	 Code ACC: 0.5057736720554272
2023-07-07 17:51:07,682 - wrong_be_tree_count:68	wrong_total:174	 wrong be tree ACC: 0.39080459770114945
2023-07-07 17:51:07,684 - save best model to ./output/test/best_model
2023-07-07 17:51:22,812 - 


2023-07-07 17:51:22,812 - epoch:21,	loss:0.42933979257941246
2023-07-07 17:51:25,993 - right_count:267	total:433	 Answer ACC: 0.6166281755196305
2023-07-07 17:51:25,994 - right_codes_count:228	total:433	 Code ACC: 0.5265588914549654
2023-07-07 17:51:25,994 - wrong_be_tree_count:52	wrong_total:166	 wrong be tree ACC: 0.3132530120481928
2023-07-07 17:51:25,996 - save best model to ./output/test/best_model
2023-07-07 17:51:41,149 - 


2023-07-07 17:51:41,149 - epoch:22,	loss:0.41038250317797065
2023-07-07 17:51:44,324 - right_count:279	total:433	 Answer ACC: 0.6443418013856813
2023-07-07 17:51:44,324 - right_codes_count:241	total:433	 Code ACC: 0.5565819861431871
2023-07-07 17:51:44,324 - wrong_be_tree_count:57	wrong_total:154	 wrong be tree ACC: 0.37012987012987014
2023-07-07 17:51:44,327 - save best model to ./output/test/best_model
2023-07-07 17:52:02,951 - 


2023-07-07 17:52:02,951 - epoch:23,	loss:0.39706104109063745
2023-07-07 17:52:06,141 - right_count:283	total:433	 Answer ACC: 0.6535796766743649
2023-07-07 17:52:06,141 - right_codes_count:242	total:433	 Code ACC: 0.558891454965358
2023-07-07 17:52:06,141 - wrong_be_tree_count:50	wrong_total:150	 wrong be tree ACC: 0.3333333333333333
2023-07-07 17:52:06,144 - save best model to ./output/test/best_model
2023-07-07 17:52:21,196 - 


2023-07-07 17:52:21,196 - epoch:24,	loss:0.36854800721630454
2023-07-07 17:52:24,347 - right_count:291	total:433	 Answer ACC: 0.6720554272517321
2023-07-07 17:52:24,347 - right_codes_count:256	total:433	 Code ACC: 0.5912240184757506
2023-07-07 17:52:24,347 - wrong_be_tree_count:50	wrong_total:142	 wrong be tree ACC: 0.352112676056338
2023-07-07 17:52:24,351 - save best model to ./output/test/best_model
2023-07-07 17:52:39,628 - 


2023-07-07 17:52:39,628 - epoch:25,	loss:0.3567523309029639
2023-07-07 17:52:42,789 - right_count:303	total:433	 Answer ACC: 0.6997690531177829
2023-07-07 17:52:42,789 - right_codes_count:270	total:433	 Code ACC: 0.6235565819861432
2023-07-07 17:52:42,789 - wrong_be_tree_count:48	wrong_total:130	 wrong be tree ACC: 0.36923076923076925
2023-07-07 17:52:42,791 - save best model to ./output/test/best_model
2023-07-07 17:52:57,942 - 


2023-07-07 17:52:57,943 - epoch:26,	loss:0.33790709218010306
2023-07-07 17:53:01,161 - right_count:310	total:433	 Answer ACC: 0.7159353348729792
2023-07-07 17:53:01,161 - right_codes_count:276	total:433	 Code ACC: 0.6374133949191686
2023-07-07 17:53:01,161 - wrong_be_tree_count:41	wrong_total:123	 wrong be tree ACC: 0.3333333333333333
2023-07-07 17:53:01,163 - save best model to ./output/test/best_model
2023-07-07 17:53:16,331 - 


2023-07-07 17:53:16,331 - epoch:27,	loss:0.3264663387089968
2023-07-07 17:53:19,572 - right_count:305	total:433	 Answer ACC: 0.7043879907621247
2023-07-07 17:53:19,572 - right_codes_count:274	total:433	 Code ACC: 0.6327944572748267
2023-07-07 17:53:19,572 - wrong_be_tree_count:49	wrong_total:128	 wrong be tree ACC: 0.3828125
2023-07-07 17:53:32,395 - 


2023-07-07 17:53:32,395 - epoch:28,	loss:0.3101033251732588
2023-07-07 17:53:35,651 - right_count:316	total:433	 Answer ACC: 0.7297921478060047
2023-07-07 17:53:35,651 - right_codes_count:286	total:433	 Code ACC: 0.6605080831408776
2023-07-07 17:53:35,651 - wrong_be_tree_count:45	wrong_total:117	 wrong be tree ACC: 0.38461538461538464
2023-07-07 17:53:35,653 - save best model to ./output/test/best_model
2023-07-07 17:53:50,701 - 


2023-07-07 17:53:50,701 - epoch:29,	loss:0.3037996394559741
2023-07-07 17:53:53,939 - right_count:320	total:433	 Answer ACC: 0.7390300230946882
2023-07-07 17:53:53,939 - right_codes_count:289	total:433	 Code ACC: 0.6674364896073903
2023-07-07 17:53:53,939 - wrong_be_tree_count:45	wrong_total:113	 wrong be tree ACC: 0.39823008849557523
2023-07-07 17:53:53,942 - save best model to ./output/test/best_model
2023-07-07 17:54:09,598 - 


2023-07-07 17:54:09,599 - epoch:30,	loss:0.29203424509614706
2023-07-07 17:54:12,741 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 17:54:12,741 - right_codes_count:296	total:433	 Code ACC: 0.6836027713625866
2023-07-07 17:54:12,741 - wrong_be_tree_count:42	wrong_total:111	 wrong be tree ACC: 0.3783783783783784
2023-07-07 17:54:12,743 - save best model to ./output/test/best_model
2023-07-07 17:54:27,759 - 


2023-07-07 17:54:27,759 - epoch:31,	loss:0.27816195227205753
2023-07-07 17:54:30,902 - right_count:323	total:433	 Answer ACC: 0.745958429561201
2023-07-07 17:54:30,902 - right_codes_count:296	total:433	 Code ACC: 0.6836027713625866
2023-07-07 17:54:30,902 - wrong_be_tree_count:42	wrong_total:110	 wrong be tree ACC: 0.38181818181818183
2023-07-07 17:54:30,905 - save best model to ./output/test/best_model
2023-07-07 17:54:46,132 - 


2023-07-07 17:54:46,133 - epoch:32,	loss:0.27030043094418943
2023-07-07 17:54:49,272 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 17:54:49,272 - right_codes_count:300	total:433	 Code ACC: 0.6928406466512702
2023-07-07 17:54:49,272 - wrong_be_tree_count:35	wrong_total:105	 wrong be tree ACC: 0.3333333333333333
2023-07-07 17:54:49,276 - save best model to ./output/test/best_model
2023-07-07 17:55:09,367 - 


2023-07-07 17:55:09,367 - epoch:33,	loss:0.25687641045078635
2023-07-07 17:55:12,569 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 17:55:12,569 - right_codes_count:295	total:433	 Code ACC: 0.6812933025404158
2023-07-07 17:55:12,569 - wrong_be_tree_count:38	wrong_total:111	 wrong be tree ACC: 0.34234234234234234
2023-07-07 17:55:22,801 - 


2023-07-07 17:55:22,802 - epoch:34,	loss:0.25302560278214514
2023-07-07 17:55:25,926 - right_count:320	total:433	 Answer ACC: 0.7390300230946882
2023-07-07 17:55:25,927 - right_codes_count:295	total:433	 Code ACC: 0.6812933025404158
2023-07-07 17:55:25,927 - wrong_be_tree_count:35	wrong_total:113	 wrong be tree ACC: 0.30973451327433627
2023-07-07 17:55:35,853 - 


2023-07-07 17:55:35,853 - epoch:35,	loss:0.24326171609573066
2023-07-07 17:55:39,017 - right_count:314	total:433	 Answer ACC: 0.7251732101616628
2023-07-07 17:55:39,017 - right_codes_count:292	total:433	 Code ACC: 0.674364896073903
2023-07-07 17:55:39,017 - wrong_be_tree_count:37	wrong_total:119	 wrong be tree ACC: 0.31092436974789917
2023-07-07 17:55:48,984 - 


2023-07-07 17:55:48,984 - epoch:36,	loss:0.23721798020415008
2023-07-07 17:55:52,098 - right_count:323	total:433	 Answer ACC: 0.745958429561201
2023-07-07 17:55:52,098 - right_codes_count:297	total:433	 Code ACC: 0.6859122401847575
2023-07-07 17:55:52,098 - wrong_be_tree_count:32	wrong_total:110	 wrong be tree ACC: 0.2909090909090909
2023-07-07 17:56:02,084 - 


2023-07-07 17:56:02,084 - epoch:37,	loss:0.22748378349933773
2023-07-07 17:56:05,236 - right_count:320	total:433	 Answer ACC: 0.7390300230946882
2023-07-07 17:56:05,236 - right_codes_count:292	total:433	 Code ACC: 0.674364896073903
2023-07-07 17:56:05,236 - wrong_be_tree_count:30	wrong_total:113	 wrong be tree ACC: 0.26548672566371684
2023-07-07 17:56:15,701 - 


2023-07-07 17:56:15,701 - epoch:38,	loss:0.22304741037078202
2023-07-07 17:56:18,867 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 17:56:18,867 - right_codes_count:299	total:433	 Code ACC: 0.6905311778290993
2023-07-07 17:56:18,867 - wrong_be_tree_count:33	wrong_total:112	 wrong be tree ACC: 0.29464285714285715
2023-07-07 17:56:31,404 - 


2023-07-07 17:56:31,404 - epoch:39,	loss:0.2187914839014411
2023-07-07 17:56:34,581 - right_count:324	total:433	 Answer ACC: 0.7482678983833718
2023-07-07 17:56:34,581 - right_codes_count:304	total:433	 Code ACC: 0.7020785219399538
2023-07-07 17:56:34,581 - wrong_be_tree_count:33	wrong_total:109	 wrong be tree ACC: 0.30275229357798167
2023-07-07 17:56:44,515 - 


2023-07-07 17:56:44,515 - epoch:40,	loss:0.21299801440909505
2023-07-07 17:56:47,715 - right_count:319	total:433	 Answer ACC: 0.7367205542725174
2023-07-07 17:56:47,715 - right_codes_count:297	total:433	 Code ACC: 0.6859122401847575
2023-07-07 17:56:47,715 - wrong_be_tree_count:30	wrong_total:114	 wrong be tree ACC: 0.2631578947368421
2023-07-07 17:56:58,018 - 


2023-07-07 17:56:58,019 - epoch:41,	loss:0.20548623404465616
2023-07-07 17:57:01,178 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 17:57:01,179 - right_codes_count:302	total:433	 Code ACC: 0.6974595842956121
2023-07-07 17:57:01,179 - wrong_be_tree_count:31	wrong_total:108	 wrong be tree ACC: 0.28703703703703703
2023-07-07 17:57:12,236 - 


2023-07-07 17:57:12,236 - epoch:42,	loss:0.2024312315043062
2023-07-07 17:57:15,458 - right_count:320	total:433	 Answer ACC: 0.7390300230946882
2023-07-07 17:57:15,458 - right_codes_count:297	total:433	 Code ACC: 0.6859122401847575
2023-07-07 17:57:15,458 - wrong_be_tree_count:36	wrong_total:113	 wrong be tree ACC: 0.3185840707964602
2023-07-07 17:57:25,562 - 


2023-07-07 17:57:25,562 - epoch:43,	loss:0.20031667686998844
2023-07-07 17:57:28,715 - right_count:324	total:433	 Answer ACC: 0.7482678983833718
2023-07-07 17:57:28,715 - right_codes_count:305	total:433	 Code ACC: 0.7043879907621247
2023-07-07 17:57:28,715 - wrong_be_tree_count:33	wrong_total:109	 wrong be tree ACC: 0.30275229357798167
2023-07-07 17:57:38,778 - 


2023-07-07 17:57:38,778 - epoch:44,	loss:0.19583963206969202
2023-07-07 17:57:41,926 - right_count:316	total:433	 Answer ACC: 0.7297921478060047
2023-07-07 17:57:41,926 - right_codes_count:298	total:433	 Code ACC: 0.6882217090069284
2023-07-07 17:57:41,926 - wrong_be_tree_count:37	wrong_total:117	 wrong be tree ACC: 0.3162393162393162
2023-07-07 17:57:53,613 - 


2023-07-07 17:57:53,614 - epoch:45,	loss:0.1905506970360875
2023-07-07 17:57:56,796 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 17:57:56,796 - right_codes_count:304	total:433	 Code ACC: 0.7020785219399538
2023-07-07 17:57:56,796 - wrong_be_tree_count:32	wrong_total:108	 wrong be tree ACC: 0.2962962962962963
2023-07-07 17:58:09,038 - 


2023-07-07 17:58:09,038 - epoch:46,	loss:0.18421968386974186
2023-07-07 17:58:12,310 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 17:58:12,310 - right_codes_count:308	total:433	 Code ACC: 0.7113163972286374
2023-07-07 17:58:12,310 - wrong_be_tree_count:33	wrong_total:107	 wrong be tree ACC: 0.308411214953271
2023-07-07 17:58:22,924 - 


2023-07-07 17:58:22,925 - epoch:47,	loss:0.18364023917820305
2023-07-07 17:58:26,257 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 17:58:26,257 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 17:58:26,257 - wrong_be_tree_count:32	wrong_total:104	 wrong be tree ACC: 0.3076923076923077
2023-07-07 17:58:26,259 - save best model to ./output/test/best_model
2023-07-07 17:58:41,825 - 


2023-07-07 17:58:41,825 - epoch:48,	loss:0.1776506695896387
2023-07-07 17:58:45,133 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 17:58:45,133 - right_codes_count:308	total:433	 Code ACC: 0.7113163972286374
2023-07-07 17:58:45,133 - wrong_be_tree_count:33	wrong_total:106	 wrong be tree ACC: 0.3113207547169811
2023-07-07 17:58:55,187 - 


2023-07-07 17:58:55,187 - epoch:49,	loss:0.1740993153071031
2023-07-07 17:58:58,445 - right_count:323	total:433	 Answer ACC: 0.745958429561201
2023-07-07 17:58:58,445 - right_codes_count:306	total:433	 Code ACC: 0.7066974595842956
2023-07-07 17:58:58,445 - wrong_be_tree_count:38	wrong_total:110	 wrong be tree ACC: 0.34545454545454546
2023-07-07 17:59:08,389 - 


2023-07-07 17:59:08,389 - epoch:50,	loss:0.1676749202888459
2023-07-07 17:59:11,714 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 17:59:11,714 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 17:59:11,714 - wrong_be_tree_count:34	wrong_total:106	 wrong be tree ACC: 0.32075471698113206
2023-07-07 17:59:21,680 - 


2023-07-07 17:59:21,680 - epoch:51,	loss:0.1662775642471388
2023-07-07 17:59:24,971 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 17:59:24,971 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 17:59:24,971 - wrong_be_tree_count:34	wrong_total:104	 wrong be tree ACC: 0.3269230769230769
2023-07-07 17:59:37,690 - 


2023-07-07 17:59:37,690 - epoch:52,	loss:0.16236144490540028
2023-07-07 17:59:41,188 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 17:59:41,188 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 17:59:41,188 - wrong_be_tree_count:35	wrong_total:104	 wrong be tree ACC: 0.33653846153846156
2023-07-07 17:59:51,863 - 


2023-07-07 17:59:51,863 - epoch:53,	loss:0.1612252553459257
2023-07-07 17:59:55,146 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 17:59:55,147 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 17:59:55,147 - wrong_be_tree_count:35	wrong_total:103	 wrong be tree ACC: 0.33980582524271846
2023-07-07 17:59:55,149 - save best model to ./output/test/best_model
2023-07-07 18:00:10,882 - 


2023-07-07 18:00:10,883 - epoch:54,	loss:0.1557168388972059
2023-07-07 18:00:14,188 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:00:14,189 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:00:14,189 - wrong_be_tree_count:34	wrong_total:105	 wrong be tree ACC: 0.3238095238095238
2023-07-07 18:00:24,217 - 


2023-07-07 18:00:24,217 - epoch:55,	loss:0.1556144873611629
2023-07-07 18:00:27,488 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 18:00:27,489 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:00:27,489 - wrong_be_tree_count:29	wrong_total:97	 wrong be tree ACC: 0.29896907216494845
2023-07-07 18:00:27,492 - save best model to ./output/test/best_model
2023-07-07 18:00:42,834 - 


2023-07-07 18:00:42,835 - epoch:56,	loss:0.1552577727707103
2023-07-07 18:00:46,215 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:00:46,215 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:00:46,215 - wrong_be_tree_count:34	wrong_total:106	 wrong be tree ACC: 0.32075471698113206
2023-07-07 18:00:57,826 - 


2023-07-07 18:00:57,826 - epoch:57,	loss:0.15080846974160522
2023-07-07 18:01:01,068 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:01:01,068 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:01:01,068 - wrong_be_tree_count:25	wrong_total:103	 wrong be tree ACC: 0.24271844660194175
2023-07-07 18:01:14,964 - 


2023-07-07 18:01:14,964 - epoch:58,	loss:0.1482559395954013
2023-07-07 18:01:18,216 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:01:18,216 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:01:18,216 - wrong_be_tree_count:28	wrong_total:103	 wrong be tree ACC: 0.27184466019417475
2023-07-07 18:01:28,232 - 


2023-07-07 18:01:28,232 - epoch:59,	loss:0.14725988055579364
2023-07-07 18:01:31,513 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:01:31,513 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:01:31,513 - wrong_be_tree_count:34	wrong_total:105	 wrong be tree ACC: 0.3238095238095238
2023-07-07 18:01:41,608 - 


2023-07-07 18:01:41,608 - epoch:60,	loss:0.1471973954467103
2023-07-07 18:01:45,072 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 18:01:45,072 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 18:01:45,072 - wrong_be_tree_count:39	wrong_total:108	 wrong be tree ACC: 0.3611111111111111
2023-07-07 18:01:55,129 - 


2023-07-07 18:01:55,130 - epoch:61,	loss:0.1440999370533973
2023-07-07 18:01:58,419 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:01:58,419 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:01:58,419 - wrong_be_tree_count:36	wrong_total:107	 wrong be tree ACC: 0.3364485981308411
2023-07-07 18:02:08,698 - 


2023-07-07 18:02:08,699 - epoch:62,	loss:0.13931934325955808
2023-07-07 18:02:11,864 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:02:11,865 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:02:11,865 - wrong_be_tree_count:29	wrong_total:103	 wrong be tree ACC: 0.2815533980582524
2023-07-07 18:02:22,006 - 


2023-07-07 18:02:22,006 - epoch:63,	loss:0.1388905413914472
2023-07-07 18:02:25,155 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:02:25,155 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:02:25,155 - wrong_be_tree_count:29	wrong_total:105	 wrong be tree ACC: 0.2761904761904762
2023-07-07 18:02:36,845 - 


2023-07-07 18:02:36,846 - epoch:64,	loss:0.13652996532619
2023-07-07 18:02:40,124 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:02:40,124 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:02:40,124 - wrong_be_tree_count:39	wrong_total:107	 wrong be tree ACC: 0.3644859813084112
2023-07-07 18:02:50,130 - 


2023-07-07 18:02:50,130 - epoch:65,	loss:0.1351269098231569
2023-07-07 18:02:53,450 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:02:53,450 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:02:53,450 - wrong_be_tree_count:28	wrong_total:103	 wrong be tree ACC: 0.27184466019417475
2023-07-07 18:03:03,459 - 


2023-07-07 18:03:03,460 - epoch:66,	loss:0.1323700798675418
2023-07-07 18:03:06,716 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 18:03:06,717 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:03:06,717 - wrong_be_tree_count:34	wrong_total:100	 wrong be tree ACC: 0.34
2023-07-07 18:03:16,781 - 


2023-07-07 18:03:16,782 - epoch:67,	loss:0.13109278003685176
2023-07-07 18:03:20,079 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:03:20,079 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:03:20,079 - wrong_be_tree_count:32	wrong_total:103	 wrong be tree ACC: 0.3106796116504854
2023-07-07 18:03:30,380 - 


2023-07-07 18:03:30,380 - epoch:68,	loss:0.130059173097834
2023-07-07 18:03:33,560 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:03:33,560 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:03:33,560 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 18:03:44,200 - 


2023-07-07 18:03:44,201 - epoch:69,	loss:0.12619090860243887
2023-07-07 18:03:47,471 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:03:47,472 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:03:47,472 - wrong_be_tree_count:37	wrong_total:102	 wrong be tree ACC: 0.3627450980392157
2023-07-07 18:04:00,036 - 


2023-07-07 18:04:00,037 - epoch:70,	loss:0.12465539324330166
2023-07-07 18:04:03,389 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:04:03,390 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:04:03,390 - wrong_be_tree_count:34	wrong_total:106	 wrong be tree ACC: 0.32075471698113206
2023-07-07 18:04:13,801 - 


2023-07-07 18:04:13,802 - epoch:71,	loss:0.12372860382311046
2023-07-07 18:04:17,091 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:04:17,091 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:04:17,091 - wrong_be_tree_count:33	wrong_total:106	 wrong be tree ACC: 0.3113207547169811
2023-07-07 18:04:27,235 - 


2023-07-07 18:04:27,236 - epoch:72,	loss:0.12381758040282875
2023-07-07 18:04:30,439 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:04:30,440 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:04:30,440 - wrong_be_tree_count:34	wrong_total:106	 wrong be tree ACC: 0.32075471698113206
2023-07-07 18:04:40,429 - 


2023-07-07 18:04:40,430 - epoch:73,	loss:0.1195546054514125
2023-07-07 18:04:43,646 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:04:43,646 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:04:43,646 - wrong_be_tree_count:29	wrong_total:107	 wrong be tree ACC: 0.27102803738317754
2023-07-07 18:04:53,686 - 


2023-07-07 18:04:53,686 - epoch:74,	loss:0.12288924551103264
2023-07-07 18:04:56,854 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:04:56,854 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:04:56,854 - wrong_be_tree_count:36	wrong_total:103	 wrong be tree ACC: 0.34951456310679613
2023-07-07 18:05:06,784 - 


2023-07-07 18:05:06,784 - epoch:75,	loss:0.11514996341429651
2023-07-07 18:05:10,057 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:05:10,057 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:05:10,057 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:05:23,262 - 


2023-07-07 18:05:23,263 - epoch:76,	loss:0.11717325437348336
2023-07-07 18:05:26,550 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:05:26,550 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:05:26,550 - wrong_be_tree_count:28	wrong_total:107	 wrong be tree ACC: 0.2616822429906542
2023-07-07 18:05:36,706 - 


2023-07-07 18:05:36,707 - epoch:77,	loss:0.11992833961267024
2023-07-07 18:05:39,953 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:05:39,953 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:05:39,953 - wrong_be_tree_count:36	wrong_total:107	 wrong be tree ACC: 0.3364485981308411
2023-07-07 18:17:51,979 - 


2023-07-07 18:17:51,979 - epoch:78,	loss:0.1169226304627955
2023-07-07 18:17:55,231 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:17:55,231 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 18:17:55,231 - wrong_be_tree_count:37	wrong_total:107	 wrong be tree ACC: 0.34579439252336447
2023-07-07 18:18:05,096 - 


2023-07-07 18:18:05,096 - epoch:79,	loss:0.11680753249675035
2023-07-07 18:18:08,213 - right_count:324	total:433	 Answer ACC: 0.7482678983833718
2023-07-07 18:18:08,213 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:18:08,213 - wrong_be_tree_count:35	wrong_total:109	 wrong be tree ACC: 0.3211009174311927
2023-07-07 18:18:18,197 - 


2023-07-07 18:18:18,198 - epoch:80,	loss:0.11468320374842733
2023-07-07 18:18:21,341 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:18:21,341 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:18:21,341 - wrong_be_tree_count:34	wrong_total:105	 wrong be tree ACC: 0.3238095238095238
2023-07-07 18:18:31,499 - 


2023-07-07 18:18:31,500 - epoch:81,	loss:0.11133100261213258
2023-07-07 18:18:34,654 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 18:18:34,654 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:18:34,654 - wrong_be_tree_count:34	wrong_total:104	 wrong be tree ACC: 0.3269230769230769
2023-07-07 18:18:44,901 - 


2023-07-07 18:18:44,901 - epoch:82,	loss:0.11485084856394678
2023-07-07 18:18:48,055 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:18:48,055 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:18:48,055 - wrong_be_tree_count:33	wrong_total:106	 wrong be tree ACC: 0.3113207547169811
2023-07-07 18:18:58,062 - 


2023-07-07 18:18:58,062 - epoch:83,	loss:0.11598824086831883
2023-07-07 18:19:01,216 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 18:19:01,216 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:19:01,216 - wrong_be_tree_count:34	wrong_total:101	 wrong be tree ACC: 0.33663366336633666
2023-07-07 18:19:11,255 - 


2023-07-07 18:19:11,255 - epoch:84,	loss:0.10824951040558517
2023-07-07 18:19:14,543 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:19:14,543 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:19:14,543 - wrong_be_tree_count:34	wrong_total:106	 wrong be tree ACC: 0.32075471698113206
2023-07-07 18:19:24,681 - 


2023-07-07 18:19:24,681 - epoch:85,	loss:0.10793459485284984
2023-07-07 18:19:28,188 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 18:19:28,188 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:19:28,188 - wrong_be_tree_count:32	wrong_total:104	 wrong be tree ACC: 0.3076923076923077
2023-07-07 18:19:38,531 - 


2023-07-07 18:19:38,531 - epoch:86,	loss:0.11018175166100264
2023-07-07 18:19:41,782 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:19:41,782 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:19:41,782 - wrong_be_tree_count:36	wrong_total:105	 wrong be tree ACC: 0.34285714285714286
2023-07-07 18:19:51,744 - 


2023-07-07 18:19:51,744 - epoch:87,	loss:0.10728412627940997
2023-07-07 18:19:54,931 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 18:19:54,932 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:19:54,932 - wrong_be_tree_count:32	wrong_total:105	 wrong be tree ACC: 0.3047619047619048
2023-07-07 18:20:05,083 - 


2023-07-07 18:20:05,083 - epoch:88,	loss:0.10846504871733487
2023-07-07 18:20:08,297 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:20:08,297 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:20:08,297 - wrong_be_tree_count:35	wrong_total:106	 wrong be tree ACC: 0.330188679245283
2023-07-07 18:20:18,446 - 


2023-07-07 18:20:18,446 - epoch:89,	loss:0.1047016930533573
2023-07-07 18:20:21,682 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 18:20:21,682 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:20:21,682 - wrong_be_tree_count:31	wrong_total:104	 wrong be tree ACC: 0.2980769230769231
2023-07-07 18:20:31,858 - 


2023-07-07 18:20:31,858 - epoch:90,	loss:0.10411932080751285
2023-07-07 18:20:35,083 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 18:20:35,084 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:20:35,084 - wrong_be_tree_count:34	wrong_total:108	 wrong be tree ACC: 0.3148148148148148
2023-07-07 18:20:44,999 - 


2023-07-07 18:20:44,999 - epoch:91,	loss:0.1044753939495422
2023-07-07 18:20:48,201 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 18:20:48,201 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 18:20:48,201 - wrong_be_tree_count:32	wrong_total:106	 wrong be tree ACC: 0.3018867924528302
2023-07-07 18:20:58,044 - 


2023-07-07 18:20:58,045 - epoch:92,	loss:0.10275945195462555
2023-07-07 18:21:01,272 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 18:21:01,272 - right_codes_count:310	total:433	 Code ACC: 0.7159353348729792
2023-07-07 18:21:01,272 - wrong_be_tree_count:34	wrong_total:108	 wrong be tree ACC: 0.3148148148148148
2023-07-07 18:21:11,249 - 


2023-07-07 18:21:11,249 - epoch:93,	loss:0.10211960755987093
2023-07-07 18:21:14,443 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:21:14,443 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 18:21:14,444 - wrong_be_tree_count:29	wrong_total:103	 wrong be tree ACC: 0.2815533980582524
2023-07-07 18:21:24,554 - 


2023-07-07 18:21:24,554 - epoch:94,	loss:0.09983924249536358
2023-07-07 18:21:27,824 - right_count:326	total:433	 Answer ACC: 0.7528868360277137
2023-07-07 18:21:27,824 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 18:21:27,824 - wrong_be_tree_count:38	wrong_total:107	 wrong be tree ACC: 0.35514018691588783
2023-07-07 18:21:38,022 - 


2023-07-07 18:21:38,023 - epoch:95,	loss:0.10167556203668937
2023-07-07 18:21:41,245 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:21:41,245 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:21:41,245 - wrong_be_tree_count:29	wrong_total:103	 wrong be tree ACC: 0.2815533980582524
2023-07-07 18:21:51,203 - 


2023-07-07 18:21:51,203 - epoch:96,	loss:0.10250774113228545
2023-07-07 18:21:54,392 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:21:54,392 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:21:54,392 - wrong_be_tree_count:32	wrong_total:103	 wrong be tree ACC: 0.3106796116504854
2023-07-07 18:22:04,222 - 


2023-07-07 18:22:04,222 - epoch:97,	loss:0.10190628271084279
2023-07-07 18:22:07,697 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:22:07,697 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:22:07,697 - wrong_be_tree_count:33	wrong_total:102	 wrong be tree ACC: 0.3235294117647059
2023-07-07 18:22:17,882 - 


2023-07-07 18:22:17,882 - epoch:98,	loss:0.0984803230385296
2023-07-07 18:22:21,162 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 18:22:21,163 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:22:21,163 - wrong_be_tree_count:33	wrong_total:104	 wrong be tree ACC: 0.3173076923076923
2023-07-07 18:22:31,319 - 


2023-07-07 18:22:31,319 - epoch:99,	loss:0.09908528497908264
2023-07-07 18:22:34,798 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 18:22:34,798 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 18:22:34,798 - wrong_be_tree_count:34	wrong_total:99	 wrong be tree ACC: 0.3434343434343434
2023-07-07 18:22:44,891 - 


2023-07-07 18:22:44,891 - epoch:100,	loss:0.0963644910370931
2023-07-07 18:22:48,255 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:22:48,255 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:22:48,255 - wrong_be_tree_count:35	wrong_total:103	 wrong be tree ACC: 0.33980582524271846
2023-07-07 18:22:58,420 - 


2023-07-07 18:22:58,420 - epoch:101,	loss:0.09708574978867546
2023-07-07 18:23:01,627 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:23:01,627 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:23:01,627 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 18:23:11,549 - 


2023-07-07 18:23:11,549 - epoch:102,	loss:0.0984720223932527
2023-07-07 18:23:14,694 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:23:14,695 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 18:23:14,695 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 18:23:24,654 - 


2023-07-07 18:23:24,655 - epoch:103,	loss:0.09920104558113962
2023-07-07 18:23:27,878 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:23:27,878 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 18:23:27,878 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 18:23:37,845 - 


2023-07-07 18:23:37,845 - epoch:104,	loss:0.10032063396647573
2023-07-07 18:23:41,031 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:23:41,031 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:23:41,031 - wrong_be_tree_count:31	wrong_total:103	 wrong be tree ACC: 0.30097087378640774
2023-07-07 18:23:50,931 - 


2023-07-07 18:23:50,932 - epoch:105,	loss:0.09813005989417434
2023-07-07 18:23:54,380 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:23:54,380 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 18:23:54,380 - wrong_be_tree_count:33	wrong_total:102	 wrong be tree ACC: 0.3235294117647059
2023-07-07 18:24:04,343 - 


2023-07-07 18:24:04,343 - epoch:106,	loss:0.09554274810943753
2023-07-07 18:24:07,557 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:24:07,557 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:24:07,557 - wrong_be_tree_count:34	wrong_total:103	 wrong be tree ACC: 0.3300970873786408
2023-07-07 18:24:17,548 - 


2023-07-07 18:24:17,548 - epoch:107,	loss:0.09603409498231485
2023-07-07 18:24:20,699 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:24:20,699 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 18:24:20,699 - wrong_be_tree_count:33	wrong_total:102	 wrong be tree ACC: 0.3235294117647059
2023-07-07 18:24:30,633 - 


2023-07-07 18:24:30,633 - epoch:108,	loss:0.094701282447204
2023-07-07 18:24:33,878 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:24:33,878 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:24:33,878 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:24:43,836 - 


2023-07-07 18:24:43,837 - epoch:109,	loss:0.0924784584203735
2023-07-07 18:24:47,047 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:24:47,048 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:24:47,048 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:24:57,519 - 


2023-07-07 18:24:57,520 - epoch:110,	loss:0.09483242995338514
2023-07-07 18:25:00,797 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:25:00,797 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:25:00,797 - wrong_be_tree_count:32	wrong_total:103	 wrong be tree ACC: 0.3106796116504854
2023-07-07 18:25:10,780 - 


2023-07-07 18:25:10,781 - epoch:111,	loss:0.09280637808842584
2023-07-07 18:25:14,444 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:25:14,444 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:25:14,444 - wrong_be_tree_count:34	wrong_total:103	 wrong be tree ACC: 0.3300970873786408
2023-07-07 18:25:24,541 - 


2023-07-07 18:25:24,541 - epoch:112,	loss:0.09753960353555158
2023-07-07 18:25:27,725 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:25:27,725 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 18:25:27,725 - wrong_be_tree_count:34	wrong_total:103	 wrong be tree ACC: 0.3300970873786408
2023-07-07 18:25:37,594 - 


2023-07-07 18:25:37,594 - epoch:113,	loss:0.09367444016970694
2023-07-07 18:25:40,726 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:25:40,726 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:25:40,726 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:25:50,744 - 


2023-07-07 18:25:50,745 - epoch:114,	loss:0.09244561300147325
2023-07-07 18:25:54,030 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:25:54,031 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:25:54,031 - wrong_be_tree_count:34	wrong_total:103	 wrong be tree ACC: 0.3300970873786408
2023-07-07 18:26:03,987 - 


2023-07-07 18:26:03,987 - epoch:115,	loss:0.09183465663227253
2023-07-07 18:26:07,323 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:26:07,323 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:26:07,323 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:26:17,396 - 


2023-07-07 18:26:17,397 - epoch:116,	loss:0.09443118941271678
2023-07-07 18:26:20,565 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:26:20,566 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:26:20,566 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:26:30,500 - 


2023-07-07 18:26:30,501 - epoch:117,	loss:0.09723862400278449
2023-07-07 18:26:33,723 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:26:33,723 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:26:33,723 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:26:43,798 - 


2023-07-07 18:26:43,799 - epoch:118,	loss:0.0922049903892912
2023-07-07 18:26:46,974 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 18:26:46,975 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 18:26:46,975 - wrong_be_tree_count:33	wrong_total:102	 wrong be tree ACC: 0.3235294117647059
2023-07-07 18:26:57,623 - 


2023-07-07 18:26:57,623 - epoch:119,	loss:0.09148652839940041
2023-07-07 18:27:00,865 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:27:00,865 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:27:00,865 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:27:10,813 - 


2023-07-07 18:27:10,813 - epoch:120,	loss:0.09378045698394999
2023-07-07 18:27:13,962 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:27:13,962 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:27:13,962 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:27:24,012 - 


2023-07-07 18:27:24,012 - epoch:121,	loss:0.09331529756309465
2023-07-07 18:27:27,168 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:27:27,168 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:27:27,168 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:27:37,109 - 


2023-07-07 18:27:37,109 - epoch:122,	loss:0.09408116660779342
2023-07-07 18:27:40,380 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:27:40,380 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:27:40,380 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:27:50,477 - 


2023-07-07 18:27:50,477 - epoch:123,	loss:0.09372458601137623
2023-07-07 18:27:53,752 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:27:53,753 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:27:53,753 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:28:03,637 - 


2023-07-07 18:28:03,637 - epoch:124,	loss:0.09028399683302268
2023-07-07 18:28:06,775 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:28:06,775 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:28:06,775 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:28:16,806 - 


2023-07-07 18:28:16,807 - epoch:125,	loss:0.09277523431228474
2023-07-07 18:28:19,973 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:28:19,974 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:28:19,974 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:28:29,918 - 


2023-07-07 18:28:29,918 - epoch:126,	loss:0.09341112710535526
2023-07-07 18:28:33,142 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:28:33,143 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:28:33,143 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:28:43,158 - 


2023-07-07 18:28:43,158 - epoch:127,	loss:0.09109844890190288
2023-07-07 18:28:46,323 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:28:46,323 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:28:46,323 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:28:56,612 - 


2023-07-07 18:28:56,612 - epoch:128,	loss:0.09024306014180183
2023-07-07 18:28:59,789 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:28:59,789 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:28:59,789 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:29:10,065 - 


2023-07-07 18:29:10,066 - epoch:129,	loss:0.09220867260592058
2023-07-07 18:29:13,390 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 18:29:13,390 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:29:13,390 - wrong_be_tree_count:33	wrong_total:103	 wrong be tree ACC: 0.32038834951456313
2023-07-07 18:29:18,454 - 


2023-07-07 18:29:18,454 - final_test
2023-07-07 18:29:22,252 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 18:29:22,253 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 18:29:22,253 - wrong_be_tree_count:36	wrong_total:104	 wrong be tree ACC: 0.34615384615384615
2023-07-07 19:16:44,982 - get train data loader...
2023-07-07 19:16:45,066 - get dev data loader...
2023-07-07 19:16:45,088 - define model...
2023-07-07 19:16:51,770 - define optimizer...
2023-07-07 19:16:51,771 - ===========================train setting parameters=========================
2023-07-07 19:16:51,771 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:16:51,772 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:16:51,772 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:16:51,772 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,772 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,772 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,772 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,772 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,772 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,773 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,773 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,773 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,773 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,773 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,774 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,774 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,774 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,775 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,776 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,776 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,776 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,776 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,776 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,777 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,777 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,778 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,779 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,779 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,780 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,780 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,780 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,783 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,783 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,784 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,785 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,787 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,787 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,787 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,787 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,788 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,788 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,788 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,790 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,790 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,791 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,791 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,791 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,791 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,792 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,792 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,792 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,793 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,793 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,793 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,794 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,794 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,795 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,796 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,796 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,796 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,796 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,796 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,796 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,796 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:16:51,797 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:16:51,798 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:16:51,798 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:16:51,798 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:16:51,798 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:16:51,798 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:16:51,798 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:16:51,799 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:16:51,799 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:16:51,799 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:16:51,799 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:16:51,799 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:16:51,799 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:16:51,800 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:16:51,800 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:16:51,800 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:16:51,800 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:16:51,800 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:16:51,800 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:16:51,802 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:16:51,802 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:16:51,802 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:16:51,802 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:16:51,802 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:16:51,802 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:16:51,803 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:16:51,803 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:16:51,803 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:16:51,803 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:16:51,805 - 
>>>>>>>>>>>>>>>>>>>start train......
2023-07-07 19:16:58,248 - 


2023-07-07 19:16:58,249 - epoch:0,	loss:1.9495259579271078
2023-07-07 19:17:01,467 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 19:17:01,467 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:17:01,467 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 19:17:01,469 - save best model to ./output/test/best_model
2023-07-07 19:26:26,181 - get train data loader...
2023-07-07 19:26:26,627 - get dev data loader...
2023-07-07 19:26:26,797 - define model...
2023-07-07 19:26:31,288 - define optimizer...
2023-07-07 19:26:31,291 - ===========================train setting parameters=========================
2023-07-07 19:26:31,291 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:26:31,291 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:26:31,292 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:26:31,292 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,292 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,293 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,293 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,293 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,293 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,294 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,295 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,295 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,295 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,295 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,296 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,296 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,296 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,297 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,297 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,297 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,298 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,299 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,300 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,300 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,301 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,301 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,301 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,301 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,301 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,302 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,303 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,303 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,303 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,305 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,305 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,305 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,306 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,306 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,307 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,307 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,307 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,308 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,308 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,309 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,309 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,309 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,310 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,310 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,310 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,310 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,310 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,311 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,311 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,311 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,311 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,312 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,312 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,312 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,313 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,313 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,313 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,314 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,314 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,315 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,315 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,315 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,315 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,315 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,316 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,317 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,317 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,318 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,318 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,318 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,319 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,319 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,320 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,320 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,321 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,321 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,321 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,321 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,322 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,322 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,323 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,324 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,324 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,324 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,325 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,325 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,325 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,326 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,326 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,326 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,327 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,327 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,327 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,328 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,328 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,329 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,329 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,329 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,330 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,330 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,330 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,330 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,331 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,331 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,331 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,332 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,333 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,333 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,333 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,333 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,334 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,334 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,335 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,335 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,335 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,336 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,337 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,337 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,337 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,338 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,338 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:26:31,338 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:26:31,339 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:26:31,339 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:26:31,340 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:26:31,340 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:26:31,340 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,342 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,342 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,342 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,342 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:26:31,343 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:26:31,343 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:26:31,343 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:26:31,343 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:26:31,344 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:26:31,344 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:26:31,344 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:26:31,344 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:26:31,344 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:26:31,344 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:26:31,344 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:26:31,345 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:26:31,345 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:26:31,345 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:26:31,346 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:26:31,346 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:26:31,346 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:26:31,347 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:26:31,347 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:26:31,347 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:26:31,348 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:26:31,348 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:26:31,348 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:26:31,349 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:26:31,349 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:26:31,349 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:26:31,350 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:26:31,350 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:26:31,350 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:27:00,443 - get train data loader...
2023-07-07 19:27:00,540 - get dev data loader...
2023-07-07 19:27:00,564 - define model...
2023-07-07 19:27:04,671 - define optimizer...
2023-07-07 19:27:04,672 - ===========================train setting parameters=========================
2023-07-07 19:27:04,672 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:27:04,672 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:27:04,672 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:27:04,672 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,673 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,673 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,673 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,674 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,674 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,674 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,674 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,674 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,675 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,675 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,675 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,675 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,675 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,676 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,676 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,676 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,676 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,676 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,677 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,678 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,678 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,678 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,678 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,678 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,679 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,680 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,680 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,680 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,680 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,680 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,681 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,681 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,681 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,681 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,682 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,683 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,683 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,685 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,686 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,686 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,686 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,686 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,686 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,686 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,686 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,686 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,687 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,688 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,689 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,689 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,689 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,689 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,689 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,689 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,689 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,689 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,689 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,690 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,690 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,690 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,690 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,690 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,691 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,691 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,691 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,692 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,692 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,692 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,692 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,692 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,692 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,693 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,694 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,695 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,696 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,696 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,696 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,696 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,696 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,696 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,696 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,697 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,698 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,698 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,699 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:27:04,700 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:27:04,701 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:27:04,701 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:27:04,701 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:27:04,701 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:27:04,701 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:27:04,701 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:27:04,701 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:27:04,701 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:27:04,701 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:27:04,701 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:27:04,701 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:27:04,701 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:27:04,701 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:27:04,701 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:27:04,701 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:27:04,701 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:27:04,701 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:27:04,701 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:27:04,702 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:27:04,702 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:27:04,702 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:27:04,702 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:27:04,702 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:27:04,702 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:27:04,702 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:27:04,702 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:27:04,706 - 


2023-07-07 19:27:04,706 - final_test
2023-07-07 19:27:08,979 - right_count:1	total:433	 Answer ACC: 0.0023094688221709007
2023-07-07 19:27:08,979 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:27:08,979 - wrong_be_tree_count:419	wrong_total:432	 wrong be tree ACC: 0.9699074074074074
2023-07-07 19:31:26,068 - get train data loader...
2023-07-07 19:31:26,153 - get dev data loader...
2023-07-07 19:31:26,180 - define model...
2023-07-07 19:31:30,078 - define optimizer...
2023-07-07 19:31:30,079 - ===========================train setting parameters=========================
2023-07-07 19:31:30,079 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:31:30,079 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:31:30,079 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:31:30,079 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,079 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,079 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,079 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,079 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,083 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,084 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,084 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,084 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,084 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,084 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,085 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,085 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,085 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,085 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,086 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,086 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,086 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,087 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,087 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,087 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,088 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,088 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,088 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,088 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,089 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,089 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,089 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,089 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,089 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,089 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,090 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,090 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,090 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,090 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,091 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,092 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,092 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,092 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,092 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,093 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,093 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,093 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,093 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,093 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,094 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,095 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,095 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,095 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,095 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,096 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,096 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,097 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,097 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,097 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,097 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,098 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,099 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,099 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,099 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,099 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,099 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,099 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,099 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,099 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,099 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,099 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,100 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,100 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,100 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,100 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,100 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,101 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,101 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,101 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,102 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,102 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,102 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,103 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,103 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,104 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,104 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,104 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,104 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,105 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,105 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,105 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,105 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,106 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,106 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,106 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,106 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,106 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,108 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,108 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,108 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,108 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,109 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,109 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,109 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,109 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,110 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,110 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,110 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,110 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,110 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,112 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,112 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,112 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,112 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,112 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,112 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,112 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,113 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,113 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,113 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,113 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,113 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,114 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,115 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,115 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,115 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,116 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,116 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,116 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,117 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,117 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,117 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,117 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,117 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,118 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,118 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,119 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,120 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,120 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:31:30,120 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:31:30,121 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:31:30,121 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:31:30,121 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:31:30,121 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:31:30,122 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,122 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,122 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,122 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,122 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:31:30,123 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:31:30,123 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:31:30,123 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:31:30,123 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:31:30,123 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:31:30,124 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:31:30,124 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:31:30,124 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:31:30,124 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:31:30,125 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:31:30,125 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:31:30,126 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:31:30,126 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:31:30,126 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:31:30,126 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:31:30,126 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:31:30,126 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:31:30,126 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:31:30,127 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:31:30,127 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:31:30,127 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:31:30,127 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:31:30,127 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:31:30,127 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:31:30,127 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:31:30,127 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:31:30,128 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:31:30,128 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:31:30,128 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:31:30,131 - 


2023-07-07 19:31:30,131 - final_test
2023-07-07 19:31:34,848 - right_count:1	total:433	 Answer ACC: 0.0023094688221709007
2023-07-07 19:31:34,848 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:31:34,848 - wrong_be_tree_count:419	wrong_total:432	 wrong be tree ACC: 0.9699074074074074
2023-07-07 19:32:14,002 - get train data loader...
2023-07-07 19:32:14,097 - get dev data loader...
2023-07-07 19:32:14,124 - define model...
2023-07-07 19:32:18,278 - define optimizer...
2023-07-07 19:32:18,279 - ===========================train setting parameters=========================
2023-07-07 19:32:18,284 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:32:18,284 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:32:18,284 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:32:18,284 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,284 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,284 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,285 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,286 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,286 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,287 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,287 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,288 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,289 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,289 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,290 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,291 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,292 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,292 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,293 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,294 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,296 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,296 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,296 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,296 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,298 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,299 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,299 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,299 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,299 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,299 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,299 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,299 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,299 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,299 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,300 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,300 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,301 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,302 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,303 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,303 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:32:18,304 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:32:18,304 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:32:18,305 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:32:18,305 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:32:18,305 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:32:18,305 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:32:18,305 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:32:18,305 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:32:18,305 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:32:18,305 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:32:18,305 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:32:18,305 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:32:18,305 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:32:18,305 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:32:18,305 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:32:18,305 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:32:18,305 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:32:18,305 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:32:18,305 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:32:18,305 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:32:18,305 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:32:18,307 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:32:18,307 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:32:18,307 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:32:18,307 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:32:18,309 - 


2023-07-07 19:32:18,309 - final_test
2023-07-07 19:32:23,027 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 19:32:23,027 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:32:23,027 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 19:33:48,273 - get train data loader...
2023-07-07 19:33:48,425 - get dev data loader...
2023-07-07 19:33:48,467 - define model...
2023-07-07 19:33:52,657 - define optimizer...
2023-07-07 19:33:52,657 - ===========================train setting parameters=========================
2023-07-07 19:33:52,658 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:33:52,658 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:33:52,658 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:33:52,658 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,658 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,658 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,658 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,659 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,660 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,660 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,660 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,660 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,660 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,660 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,660 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,660 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,661 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,661 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,661 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,661 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,661 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,662 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,662 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,662 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,663 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,663 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,663 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,663 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,663 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,663 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,663 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,663 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,664 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,665 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,665 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,665 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,665 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,665 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,666 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,667 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,667 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,668 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,669 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,669 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,669 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,669 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,669 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,669 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,669 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,670 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,671 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,672 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,672 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,672 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,672 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,673 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,675 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,675 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,676 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,676 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,677 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,678 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,678 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,678 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,678 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,679 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,679 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,679 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,679 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,680 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:33:52,681 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:33:52,681 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:33:52,681 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:33:52,682 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:33:52,682 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:33:52,682 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:33:52,682 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:33:52,682 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:33:52,682 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:33:52,682 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:33:52,682 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:33:52,682 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:33:52,682 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:33:52,682 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:33:52,682 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:33:52,682 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:33:52,682 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:33:52,683 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:33:52,683 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:33:52,683 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:33:52,683 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:33:52,683 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:33:52,683 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:33:52,683 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:33:52,683 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:33:52,687 - 
>>>>>>>>>>>>>>>>>>>start train......
2023-07-07 19:33:58,807 - 


2023-07-07 19:33:58,808 - epoch:0,	loss:2.0519168376922607
2023-07-07 19:34:02,805 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 19:34:02,806 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:34:02,806 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 19:34:02,808 - save best model to ./output/test/best_model
2023-07-07 19:34:18,380 - 


2023-07-07 19:34:18,381 - epoch:1,	loss:1.2165446728467941
2023-07-07 19:34:22,171 - right_count:0	total:433	 Answer ACC: 0.0
2023-07-07 19:34:22,172 - right_codes_count:0	total:433	 Code ACC: 0.0
2023-07-07 19:34:22,172 - wrong_be_tree_count:433	wrong_total:433	 wrong be tree ACC: 1.0
2023-07-07 19:34:32,399 - 


2023-07-07 19:34:32,399 - epoch:2,	loss:0.9445097204297781
2023-07-07 19:34:35,929 - right_count:3	total:433	 Answer ACC: 0.006928406466512702
2023-07-07 19:34:35,930 - right_codes_count:1	total:433	 Code ACC: 0.0023094688221709007
2023-07-07 19:34:35,931 - wrong_be_tree_count:407	wrong_total:430	 wrong be tree ACC: 0.9465116279069767
2023-07-07 19:34:35,934 - save best model to ./output/test/best_model
2023-07-07 19:34:51,352 - 


2023-07-07 19:34:51,353 - epoch:3,	loss:0.8726440165191889
2023-07-07 19:34:54,975 - right_count:16	total:433	 Answer ACC: 0.03695150115473441
2023-07-07 19:34:54,976 - right_codes_count:6	total:433	 Code ACC: 0.013856812933025405
2023-07-07 19:34:54,976 - wrong_be_tree_count:298	wrong_total:417	 wrong be tree ACC: 0.7146282973621103
2023-07-07 19:34:54,978 - save best model to ./output/test/best_model
2023-07-07 19:35:10,333 - 


2023-07-07 19:35:10,333 - epoch:4,	loss:0.8309638071805239
2023-07-07 19:35:14,236 - right_count:38	total:433	 Answer ACC: 0.08775981524249422
2023-07-07 19:35:14,237 - right_codes_count:17	total:433	 Code ACC: 0.03926096997690531
2023-07-07 19:35:14,237 - wrong_be_tree_count:274	wrong_total:395	 wrong be tree ACC: 0.6936708860759494
2023-07-07 19:35:14,242 - save best model to ./output/test/best_model
2023-07-07 19:35:29,550 - 


2023-07-07 19:35:29,550 - epoch:5,	loss:0.7833625329658389
2023-07-07 19:35:33,537 - right_count:89	total:433	 Answer ACC: 0.20554272517321015
2023-07-07 19:35:33,537 - right_codes_count:62	total:433	 Code ACC: 0.14318706697459585
2023-07-07 19:35:33,537 - wrong_be_tree_count:174	wrong_total:344	 wrong be tree ACC: 0.5058139534883721
2023-07-07 19:35:33,539 - save best model to ./output/test/best_model
2023-07-07 19:35:48,771 - 


2023-07-07 19:35:48,772 - epoch:6,	loss:0.7147971233353019
2023-07-07 19:35:52,705 - right_count:101	total:433	 Answer ACC: 0.23325635103926096
2023-07-07 19:35:52,705 - right_codes_count:76	total:433	 Code ACC: 0.17551963048498845
2023-07-07 19:35:52,705 - wrong_be_tree_count:88	wrong_total:332	 wrong be tree ACC: 0.26506024096385544
2023-07-07 19:35:52,708 - save best model to ./output/test/best_model
2023-07-07 19:36:10,448 - 


2023-07-07 19:36:10,449 - epoch:7,	loss:0.6582977436482906
2023-07-07 19:36:14,136 - right_count:143	total:433	 Answer ACC: 0.3302540415704388
2023-07-07 19:36:14,136 - right_codes_count:113	total:433	 Code ACC: 0.26096997690531176
2023-07-07 19:36:14,136 - wrong_be_tree_count:58	wrong_total:290	 wrong be tree ACC: 0.2
2023-07-07 19:36:14,138 - save best model to ./output/test/best_model
2023-07-07 19:36:30,354 - 


2023-07-07 19:36:30,354 - epoch:8,	loss:0.6172377234324813
2023-07-07 19:36:34,098 - right_count:165	total:433	 Answer ACC: 0.3810623556581986
2023-07-07 19:36:34,098 - right_codes_count:133	total:433	 Code ACC: 0.3071593533487298
2023-07-07 19:36:34,098 - wrong_be_tree_count:45	wrong_total:268	 wrong be tree ACC: 0.16791044776119404
2023-07-07 19:36:34,100 - save best model to ./output/test/best_model
2023-07-07 19:36:51,390 - 


2023-07-07 19:36:51,390 - epoch:9,	loss:0.5681507019326091
2023-07-07 19:36:55,196 - right_count:181	total:433	 Answer ACC: 0.418013856812933
2023-07-07 19:36:55,196 - right_codes_count:147	total:433	 Code ACC: 0.3394919168591224
2023-07-07 19:36:55,196 - wrong_be_tree_count:43	wrong_total:252	 wrong be tree ACC: 0.17063492063492064
2023-07-07 19:36:55,198 - save best model to ./output/test/best_model
2023-07-07 19:37:10,738 - 


2023-07-07 19:37:10,739 - epoch:10,	loss:0.5249695698730648
2023-07-07 19:37:14,493 - right_count:213	total:433	 Answer ACC: 0.49191685912240185
2023-07-07 19:37:14,493 - right_codes_count:174	total:433	 Code ACC: 0.4018475750577367
2023-07-07 19:37:14,493 - wrong_be_tree_count:43	wrong_total:220	 wrong be tree ACC: 0.19545454545454546
2023-07-07 19:37:14,496 - save best model to ./output/test/best_model
2023-07-07 19:37:29,876 - 


2023-07-07 19:37:29,876 - epoch:11,	loss:0.4772184635512531
2023-07-07 19:37:33,883 - right_count:223	total:433	 Answer ACC: 0.5150115473441108
2023-07-07 19:37:33,883 - right_codes_count:186	total:433	 Code ACC: 0.4295612009237875
2023-07-07 19:37:33,884 - wrong_be_tree_count:53	wrong_total:210	 wrong be tree ACC: 0.2523809523809524
2023-07-07 19:37:33,886 - save best model to ./output/test/best_model
2023-07-07 19:37:49,192 - 


2023-07-07 19:37:49,192 - epoch:12,	loss:0.43800947442650795
2023-07-07 19:37:52,747 - right_count:246	total:433	 Answer ACC: 0.5681293302540416
2023-07-07 19:37:52,747 - right_codes_count:214	total:433	 Code ACC: 0.4942263279445728
2023-07-07 19:37:52,747 - wrong_be_tree_count:62	wrong_total:187	 wrong be tree ACC: 0.3315508021390374
2023-07-07 19:37:52,750 - save best model to ./output/test/best_model
2023-07-07 19:38:09,180 - 


2023-07-07 19:38:09,181 - epoch:13,	loss:0.3986371294595301
2023-07-07 19:38:12,742 - right_count:252	total:433	 Answer ACC: 0.581986143187067
2023-07-07 19:38:12,742 - right_codes_count:219	total:433	 Code ACC: 0.5057736720554272
2023-07-07 19:38:12,742 - wrong_be_tree_count:64	wrong_total:181	 wrong be tree ACC: 0.35359116022099446
2023-07-07 19:38:12,744 - save best model to ./output/test/best_model
2023-07-07 19:38:28,118 - 


2023-07-07 19:38:28,118 - epoch:14,	loss:0.36370793893001974
2023-07-07 19:38:31,645 - right_count:261	total:433	 Answer ACC: 0.6027713625866051
2023-07-07 19:38:31,645 - right_codes_count:231	total:433	 Code ACC: 0.5334872979214781
2023-07-07 19:38:31,645 - wrong_be_tree_count:64	wrong_total:172	 wrong be tree ACC: 0.37209302325581395
2023-07-07 19:38:31,647 - save best model to ./output/test/best_model
2023-07-07 19:38:47,344 - 


2023-07-07 19:38:47,344 - epoch:15,	loss:0.3405845477245748
2023-07-07 19:38:50,701 - right_count:283	total:433	 Answer ACC: 0.6535796766743649
2023-07-07 19:38:50,701 - right_codes_count:254	total:433	 Code ACC: 0.5866050808314087
2023-07-07 19:38:50,701 - wrong_be_tree_count:56	wrong_total:150	 wrong be tree ACC: 0.37333333333333335
2023-07-07 19:38:50,704 - save best model to ./output/test/best_model
2023-07-07 19:39:05,924 - 


2023-07-07 19:39:05,924 - epoch:16,	loss:0.32082445034757257
2023-07-07 19:39:09,173 - right_count:287	total:433	 Answer ACC: 0.6628175519630485
2023-07-07 19:39:09,173 - right_codes_count:262	total:433	 Code ACC: 0.605080831408776
2023-07-07 19:39:09,173 - wrong_be_tree_count:53	wrong_total:146	 wrong be tree ACC: 0.363013698630137
2023-07-07 19:39:09,175 - save best model to ./output/test/best_model
2023-07-07 19:39:24,456 - 


2023-07-07 19:39:24,456 - epoch:17,	loss:0.29763356293551624
2023-07-07 19:39:27,668 - right_count:295	total:433	 Answer ACC: 0.6812933025404158
2023-07-07 19:39:27,668 - right_codes_count:266	total:433	 Code ACC: 0.6143187066974596
2023-07-07 19:39:27,668 - wrong_be_tree_count:50	wrong_total:138	 wrong be tree ACC: 0.36231884057971014
2023-07-07 19:39:27,671 - save best model to ./output/test/best_model
2023-07-07 19:39:43,070 - 


2023-07-07 19:39:43,071 - epoch:18,	loss:0.2859751204960048
2023-07-07 19:39:46,254 - right_count:306	total:433	 Answer ACC: 0.7066974595842956
2023-07-07 19:39:46,254 - right_codes_count:276	total:433	 Code ACC: 0.6374133949191686
2023-07-07 19:39:46,254 - wrong_be_tree_count:49	wrong_total:127	 wrong be tree ACC: 0.3858267716535433
2023-07-07 19:39:46,256 - save best model to ./output/test/best_model
2023-07-07 19:40:01,497 - 


2023-07-07 19:40:01,498 - epoch:19,	loss:0.2583670390304178
2023-07-07 19:40:04,669 - right_count:310	total:433	 Answer ACC: 0.7159353348729792
2023-07-07 19:40:04,670 - right_codes_count:285	total:433	 Code ACC: 0.6581986143187067
2023-07-07 19:40:04,670 - wrong_be_tree_count:52	wrong_total:123	 wrong be tree ACC: 0.42276422764227645
2023-07-07 19:40:04,673 - save best model to ./output/test/best_model
2023-07-07 19:40:19,848 - 


2023-07-07 19:40:19,848 - epoch:20,	loss:0.24294447572901845
2023-07-07 19:40:23,047 - right_count:313	total:433	 Answer ACC: 0.7228637413394919
2023-07-07 19:40:23,047 - right_codes_count:287	total:433	 Code ACC: 0.6628175519630485
2023-07-07 19:40:23,048 - wrong_be_tree_count:49	wrong_total:120	 wrong be tree ACC: 0.4083333333333333
2023-07-07 19:40:23,051 - save best model to ./output/test/best_model
2023-07-07 19:40:38,393 - 


2023-07-07 19:40:38,394 - epoch:21,	loss:0.23250635946169496
2023-07-07 19:40:41,506 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 19:40:41,506 - right_codes_count:292	total:433	 Code ACC: 0.674364896073903
2023-07-07 19:40:41,506 - wrong_be_tree_count:44	wrong_total:112	 wrong be tree ACC: 0.39285714285714285
2023-07-07 19:40:41,509 - save best model to ./output/test/best_model
2023-07-07 19:40:56,686 - 


2023-07-07 19:40:56,687 - epoch:22,	loss:0.22407180489972234
2023-07-07 19:41:00,129 - right_count:310	total:433	 Answer ACC: 0.7159353348729792
2023-07-07 19:41:00,130 - right_codes_count:290	total:433	 Code ACC: 0.6697459584295612
2023-07-07 19:41:00,130 - wrong_be_tree_count:50	wrong_total:123	 wrong be tree ACC: 0.4065040650406504
2023-07-07 19:41:10,065 - 


2023-07-07 19:41:10,065 - epoch:23,	loss:0.21392290573567152
2023-07-07 19:41:13,205 - right_count:314	total:433	 Answer ACC: 0.7251732101616628
2023-07-07 19:41:13,206 - right_codes_count:290	total:433	 Code ACC: 0.6697459584295612
2023-07-07 19:41:13,206 - wrong_be_tree_count:44	wrong_total:119	 wrong be tree ACC: 0.3697478991596639
2023-07-07 19:41:23,440 - 


2023-07-07 19:41:23,440 - epoch:24,	loss:0.19688142545055598
2023-07-07 19:41:26,771 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 19:41:26,771 - right_codes_count:295	total:433	 Code ACC: 0.6812933025404158
2023-07-07 19:41:26,771 - wrong_be_tree_count:39	wrong_total:111	 wrong be tree ACC: 0.35135135135135137
2023-07-07 19:41:26,774 - save best model to ./output/test/best_model
2023-07-07 19:41:41,814 - 


2023-07-07 19:41:41,814 - epoch:25,	loss:0.19083242170745507
2023-07-07 19:41:45,156 - right_count:319	total:433	 Answer ACC: 0.7367205542725174
2023-07-07 19:41:45,156 - right_codes_count:299	total:433	 Code ACC: 0.6905311778290993
2023-07-07 19:41:45,156 - wrong_be_tree_count:41	wrong_total:114	 wrong be tree ACC: 0.35964912280701755
2023-07-07 19:41:55,157 - 


2023-07-07 19:41:55,157 - epoch:26,	loss:0.18387456017080694
2023-07-07 19:41:58,313 - right_count:319	total:433	 Answer ACC: 0.7367205542725174
2023-07-07 19:41:58,313 - right_codes_count:298	total:433	 Code ACC: 0.6882217090069284
2023-07-07 19:41:58,313 - wrong_be_tree_count:38	wrong_total:114	 wrong be tree ACC: 0.3333333333333333
2023-07-07 19:42:08,485 - 


2023-07-07 19:42:08,486 - epoch:27,	loss:0.1787454665172845
2023-07-07 19:42:11,720 - right_count:320	total:433	 Answer ACC: 0.7390300230946882
2023-07-07 19:42:11,720 - right_codes_count:300	total:433	 Code ACC: 0.6928406466512702
2023-07-07 19:42:11,720 - wrong_be_tree_count:40	wrong_total:113	 wrong be tree ACC: 0.35398230088495575
2023-07-07 19:42:21,964 - 


2023-07-07 19:42:21,964 - epoch:28,	loss:0.17146770062390715
2023-07-07 19:42:25,220 - right_count:322	total:433	 Answer ACC: 0.74364896073903
2023-07-07 19:42:25,220 - right_codes_count:303	total:433	 Code ACC: 0.6997690531177829
2023-07-07 19:42:25,220 - wrong_be_tree_count:38	wrong_total:111	 wrong be tree ACC: 0.34234234234234234
2023-07-07 19:42:35,258 - 


2023-07-07 19:42:35,258 - epoch:29,	loss:0.17082448676228523
2023-07-07 19:42:38,395 - right_count:321	total:433	 Answer ACC: 0.7413394919168591
2023-07-07 19:42:38,396 - right_codes_count:301	total:433	 Code ACC: 0.6951501154734411
2023-07-07 19:42:38,396 - wrong_be_tree_count:32	wrong_total:112	 wrong be tree ACC: 0.2857142857142857
2023-07-07 19:42:46,818 - get train data loader...
2023-07-07 19:42:46,903 - get dev data loader...
2023-07-07 19:42:46,971 - define model...
2023-07-07 19:42:48,505 - 


2023-07-07 19:42:48,505 - epoch:30,	loss:0.16978819447103888
2023-07-07 19:42:51,146 - define optimizer...
2023-07-07 19:42:51,147 - ===========================train setting parameters=========================
2023-07-07 19:42:51,149 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:42:51,149 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:42:51,149 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:42:51,149 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,149 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,149 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,150 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,150 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,151 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,152 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,153 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,153 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,155 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,156 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,156 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,158 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,159 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,160 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,160 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,160 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,160 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,160 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,160 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,160 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,161 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,162 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,162 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,162 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,162 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,162 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,162 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,162 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,162 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,162 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,162 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,163 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,163 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,164 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,165 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,166 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,166 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,166 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,166 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,167 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,168 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,168 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,168 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,168 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,169 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,169 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,170 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:42:51,171 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:42:51,172 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:42:51,172 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:42:51,172 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:42:51,172 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:42:51,172 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:42:51,172 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:42:51,172 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:42:51,173 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:42:51,173 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:42:51,173 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:42:51,173 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:42:51,173 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:42:51,173 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:42:51,173 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:42:51,174 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:42:51,174 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:42:51,174 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:42:51,174 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:42:51,174 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:42:51,174 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:42:51,174 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:42:51,175 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:42:51,175 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:42:51,175 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:42:51,738 - right_count:319	total:433	 Answer ACC: 0.7367205542725174
2023-07-07 19:42:51,738 - right_codes_count:301	total:433	 Code ACC: 0.6951501154734411
2023-07-07 19:42:51,738 - wrong_be_tree_count:36	wrong_total:114	 wrong be tree ACC: 0.3157894736842105
2023-07-07 19:43:01,854 - 


2023-07-07 19:43:01,855 - epoch:31,	loss:0.15827414084924385
2023-07-07 19:43:05,157 - right_count:323	total:433	 Answer ACC: 0.745958429561201
2023-07-07 19:43:05,157 - right_codes_count:302	total:433	 Code ACC: 0.6974595842956121
2023-07-07 19:43:05,157 - wrong_be_tree_count:29	wrong_total:110	 wrong be tree ACC: 0.2636363636363636
2023-07-07 19:43:05,159 - save best model to ./output/test/best_model
2023-07-07 19:43:20,255 - 


2023-07-07 19:43:20,255 - epoch:32,	loss:0.15356056671589613
2023-07-07 19:43:23,518 - right_count:324	total:433	 Answer ACC: 0.7482678983833718
2023-07-07 19:43:23,518 - right_codes_count:304	total:433	 Code ACC: 0.7020785219399538
2023-07-07 19:43:23,518 - wrong_be_tree_count:27	wrong_total:109	 wrong be tree ACC: 0.24770642201834864
2023-07-07 19:43:23,521 - save best model to ./output/test/best_model
2023-07-07 19:43:39,324 - 


2023-07-07 19:43:39,325 - epoch:33,	loss:0.14902539970353246
2023-07-07 19:43:42,750 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 19:43:42,750 - right_codes_count:304	total:433	 Code ACC: 0.7020785219399538
2023-07-07 19:43:42,750 - wrong_be_tree_count:36	wrong_total:106	 wrong be tree ACC: 0.33962264150943394
2023-07-07 19:43:42,753 - save best model to ./output/test/best_model
2023-07-07 19:43:57,832 - 


2023-07-07 19:43:57,832 - epoch:34,	loss:0.14754657173762098
2023-07-07 19:44:01,099 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 19:44:01,099 - right_codes_count:306	total:433	 Code ACC: 0.7066974595842956
2023-07-07 19:44:01,099 - wrong_be_tree_count:35	wrong_total:108	 wrong be tree ACC: 0.32407407407407407
2023-07-07 19:44:11,130 - 


2023-07-07 19:44:11,130 - epoch:35,	loss:0.14238854823634028
2023-07-07 19:44:14,425 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 19:44:14,425 - right_codes_count:306	total:433	 Code ACC: 0.7066974595842956
2023-07-07 19:44:14,425 - wrong_be_tree_count:33	wrong_total:108	 wrong be tree ACC: 0.3055555555555556
2023-07-07 19:44:24,653 - 


2023-07-07 19:44:24,654 - epoch:36,	loss:0.13948303018696606
2023-07-07 19:44:27,828 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 19:44:27,828 - right_codes_count:307	total:433	 Code ACC: 0.7090069284064665
2023-07-07 19:44:27,828 - wrong_be_tree_count:38	wrong_total:105	 wrong be tree ACC: 0.3619047619047619
2023-07-07 19:44:27,831 - save best model to ./output/test/best_model
2023-07-07 19:44:43,096 - 


2023-07-07 19:44:43,097 - epoch:37,	loss:0.13556845107814297
2023-07-07 19:44:46,304 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:44:46,304 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 19:44:46,304 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 19:44:46,306 - save best model to ./output/test/best_model
2023-07-07 19:45:01,266 - 


2023-07-07 19:45:01,267 - epoch:38,	loss:0.1325716214487329
2023-07-07 19:45:04,462 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 19:45:04,462 - right_codes_count:307	total:433	 Code ACC: 0.7090069284064665
2023-07-07 19:45:04,462 - wrong_be_tree_count:37	wrong_total:105	 wrong be tree ACC: 0.3523809523809524
2023-07-07 19:45:14,892 - 


2023-07-07 19:45:14,892 - epoch:39,	loss:0.1324873857665807
2023-07-07 19:45:18,104 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 19:45:18,105 - right_codes_count:307	total:433	 Code ACC: 0.7090069284064665
2023-07-07 19:45:18,105 - wrong_be_tree_count:39	wrong_total:104	 wrong be tree ACC: 0.375
2023-07-07 19:45:28,106 - 


2023-07-07 19:45:28,106 - epoch:40,	loss:0.12953970930539072
2023-07-07 19:45:31,303 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 19:45:31,303 - right_codes_count:306	total:433	 Code ACC: 0.7066974595842956
2023-07-07 19:45:31,303 - wrong_be_tree_count:39	wrong_total:106	 wrong be tree ACC: 0.36792452830188677
2023-07-07 19:45:41,399 - 


2023-07-07 19:45:41,399 - epoch:41,	loss:0.12475967028876767
2023-07-07 19:45:44,697 - right_count:325	total:433	 Answer ACC: 0.7505773672055427
2023-07-07 19:45:44,697 - right_codes_count:305	total:433	 Code ACC: 0.7043879907621247
2023-07-07 19:45:44,697 - wrong_be_tree_count:38	wrong_total:108	 wrong be tree ACC: 0.35185185185185186
2023-07-07 19:45:54,792 - 


2023-07-07 19:45:54,792 - epoch:42,	loss:0.127818544046022
2023-07-07 19:45:58,032 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 19:45:58,032 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 19:45:58,032 - wrong_be_tree_count:38	wrong_total:103	 wrong be tree ACC: 0.36893203883495146
2023-07-07 19:46:07,930 - 


2023-07-07 19:46:07,930 - epoch:43,	loss:0.12276539241429418
2023-07-07 19:46:11,181 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:46:11,182 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 19:46:11,182 - wrong_be_tree_count:34	wrong_total:102	 wrong be tree ACC: 0.3333333333333333
2023-07-07 19:46:21,444 - 


2023-07-07 19:46:21,444 - epoch:44,	loss:0.11700977146392688
2023-07-07 19:46:24,759 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:46:24,759 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 19:46:24,759 - wrong_be_tree_count:44	wrong_total:102	 wrong be tree ACC: 0.43137254901960786
2023-07-07 19:46:34,959 - 


2023-07-07 19:46:34,959 - epoch:45,	loss:0.11815006501274183
2023-07-07 19:46:38,285 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 19:46:38,286 - right_codes_count:311	total:433	 Code ACC: 0.7182448036951501
2023-07-07 19:46:38,286 - wrong_be_tree_count:35	wrong_total:103	 wrong be tree ACC: 0.33980582524271846
2023-07-07 19:46:48,371 - 


2023-07-07 19:46:48,372 - epoch:46,	loss:0.11535027844365686
2023-07-07 19:46:51,912 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:46:51,912 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 19:46:51,912 - wrong_be_tree_count:42	wrong_total:102	 wrong be tree ACC: 0.4117647058823529
2023-07-07 19:47:02,120 - 


2023-07-07 19:47:02,120 - epoch:47,	loss:0.11691989831160754
2023-07-07 19:47:05,864 - right_count:328	total:433	 Answer ACC: 0.7575057736720554
2023-07-07 19:47:05,864 - right_codes_count:312	total:433	 Code ACC: 0.7205542725173211
2023-07-07 19:47:05,865 - wrong_be_tree_count:38	wrong_total:105	 wrong be tree ACC: 0.3619047619047619
2023-07-07 19:47:16,077 - 


2023-07-07 19:47:16,077 - epoch:48,	loss:0.11137629637960345
2023-07-07 19:47:19,268 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:47:19,268 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:47:19,268 - wrong_be_tree_count:33	wrong_total:100	 wrong be tree ACC: 0.33
2023-07-07 19:47:19,270 - save best model to ./output/test/best_model
2023-07-07 19:47:30,339 - get train data loader...
2023-07-07 19:47:30,465 - get dev data loader...
2023-07-07 19:47:30,556 - define model...
2023-07-07 19:47:34,738 - 


2023-07-07 19:47:34,738 - epoch:49,	loss:0.10999876371352002
2023-07-07 19:47:35,378 - define optimizer...
2023-07-07 19:47:35,378 - ===========================train setting parameters=========================
2023-07-07 19:47:35,379 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:47:35,379 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:47:35,379 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:47:35,379 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,379 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,379 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,379 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,379 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,380 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,381 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,382 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,382 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,382 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,382 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,382 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,383 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,383 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,383 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,383 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,383 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,384 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,385 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,385 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,385 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,386 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,387 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,387 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,387 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,387 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,388 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,388 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,389 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,406 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,406 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,406 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,406 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,419 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,419 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,419 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,421 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,425 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,425 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,425 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,425 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,425 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,426 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,426 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,426 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,427 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,428 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,428 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,428 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,428 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,428 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,428 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,428 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,428 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,429 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,431 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,431 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,431 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,431 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,431 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,431 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,432 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,433 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,434 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,434 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,435 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,435 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,435 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,435 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,436 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,436 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,436 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,437 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,437 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,437 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,437 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:47:35,437 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:47:35,437 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:47:35,437 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:47:35,437 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:47:35,438 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:47:35,439 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:47:35,439 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:47:35,439 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:47:35,439 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:47:35,439 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:47:35,439 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:47:35,440 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:47:35,440 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:47:35,440 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:47:35,440 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:47:35,440 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:47:35,440 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:47:35,440 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:47:35,441 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:47:35,442 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:47:35,442 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:47:35,442 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:47:35,442 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:47:35,442 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:47:35,442 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:47:35,443 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:47:35,443 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:47:35,443 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:47:35,443 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:47:35,443 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:47:35,443 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:47:35,443 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:47:35,443 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:47:38,906 - right_count:327	total:433	 Answer ACC: 0.7551963048498845
2023-07-07 19:47:38,906 - right_codes_count:309	total:433	 Code ACC: 0.7136258660508084
2023-07-07 19:47:38,906 - wrong_be_tree_count:39	wrong_total:106	 wrong be tree ACC: 0.36792452830188677
2023-07-07 19:47:49,385 - 


2023-07-07 19:47:49,385 - epoch:50,	loss:0.10595832581748255
2023-07-07 19:47:52,682 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:47:52,682 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:47:52,682 - wrong_be_tree_count:37	wrong_total:101	 wrong be tree ACC: 0.36633663366336633
2023-07-07 19:48:02,799 - 


2023-07-07 19:48:02,799 - epoch:51,	loss:0.10426564863882959
2023-07-07 19:48:06,950 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 19:48:06,950 - right_codes_count:313	total:433	 Code ACC: 0.7228637413394919
2023-07-07 19:48:06,950 - wrong_be_tree_count:38	wrong_total:103	 wrong be tree ACC: 0.36893203883495146
2023-07-07 19:48:17,216 - 


2023-07-07 19:48:17,217 - epoch:52,	loss:0.10264708334580064
2023-07-07 19:48:21,273 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:48:21,273 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:48:21,273 - wrong_be_tree_count:36	wrong_total:100	 wrong be tree ACC: 0.36
2023-07-07 19:48:31,570 - 


2023-07-07 19:48:31,571 - epoch:53,	loss:0.10362558154156432
2023-07-07 19:48:34,931 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:48:34,931 - right_codes_count:314	total:433	 Code ACC: 0.7251732101616628
2023-07-07 19:48:34,931 - wrong_be_tree_count:38	wrong_total:98	 wrong be tree ACC: 0.3877551020408163
2023-07-07 19:48:34,933 - save best model to ./output/test/best_model
2023-07-07 19:48:51,925 - 


2023-07-07 19:48:51,926 - epoch:54,	loss:0.10025476309238002
2023-07-07 19:48:55,029 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:48:55,029 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:48:55,029 - wrong_be_tree_count:36	wrong_total:99	 wrong be tree ACC: 0.36363636363636365
2023-07-07 19:49:05,084 - 


2023-07-07 19:49:05,084 - epoch:55,	loss:0.09929108951473609
2023-07-07 19:49:08,206 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:49:08,206 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:49:08,206 - wrong_be_tree_count:33	wrong_total:96	 wrong be tree ACC: 0.34375
2023-07-07 19:49:08,231 - save best model to ./output/test/best_model
2023-07-07 19:49:34,278 - 


2023-07-07 19:49:34,293 - epoch:56,	loss:0.09991729212924838
2023-07-07 19:49:37,367 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:49:37,368 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:49:37,368 - wrong_be_tree_count:39	wrong_total:98	 wrong be tree ACC: 0.3979591836734694
2023-07-07 19:49:56,511 - 


2023-07-07 19:49:56,511 - epoch:57,	loss:0.09480209252797067
2023-07-07 19:49:59,556 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:49:59,556 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:49:59,556 - wrong_be_tree_count:37	wrong_total:98	 wrong be tree ACC: 0.37755102040816324
2023-07-07 19:50:14,954 - 


2023-07-07 19:50:14,955 - epoch:58,	loss:0.09629334032069892
2023-07-07 19:50:17,981 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:50:17,981 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:50:17,981 - wrong_be_tree_count:39	wrong_total:101	 wrong be tree ACC: 0.38613861386138615
2023-07-07 19:50:30,455 - 


2023-07-07 19:50:30,455 - epoch:59,	loss:0.09342200082028285
2023-07-07 19:50:33,909 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:50:33,909 - right_codes_count:315	total:433	 Code ACC: 0.7274826789838337
2023-07-07 19:50:33,909 - wrong_be_tree_count:38	wrong_total:101	 wrong be tree ACC: 0.37623762376237624
2023-07-07 19:50:44,983 - 


2023-07-07 19:50:44,983 - epoch:60,	loss:0.09612503444077447
2023-07-07 19:50:48,798 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:50:48,798 - right_codes_count:322	total:433	 Code ACC: 0.74364896073903
2023-07-07 19:50:48,798 - wrong_be_tree_count:35	wrong_total:98	 wrong be tree ACC: 0.35714285714285715
2023-07-07 19:50:55,636 - get train data loader...
2023-07-07 19:50:55,728 - get dev data loader...
2023-07-07 19:50:55,806 - define model...
2023-07-07 19:50:59,194 - 


2023-07-07 19:50:59,194 - epoch:61,	loss:0.09342715347884223
2023-07-07 19:51:00,172 - define optimizer...
2023-07-07 19:51:00,174 - ===========================train setting parameters=========================
2023-07-07 19:51:00,174 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:51:00,174 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:51:00,174 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:51:00,174 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,174 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,174 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,175 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,175 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,175 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,178 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,178 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,179 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,179 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,179 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,183 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,183 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,183 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,185 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,185 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,185 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,185 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,189 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,189 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,192 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,192 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,192 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,193 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,193 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,193 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,194 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,194 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,194 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,195 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,195 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,195 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,198 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,198 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,198 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,198 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,201 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,202 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,202 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,203 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,203 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,206 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,206 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,207 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,207 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,207 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,210 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,210 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,210 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,211 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,211 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,221 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,221 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,221 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,221 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,221 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,228 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,228 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,238 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,238 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,239 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,239 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,239 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,240 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,243 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,246 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,247 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,247 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,247 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,247 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,247 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,255 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,264 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,267 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,267 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,271 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,274 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,274 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,288 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,296 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,296 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,311 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,311 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,312 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,312 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,312 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,315 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,318 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,318 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,320 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,320 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,320 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,329 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,329 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,329 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,329 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,329 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,336 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,336 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,336 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,336 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,337 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,339 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,342 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,342 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,342 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,342 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,342 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,343 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,343 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,343 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:00,343 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:00,344 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:00,344 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:00,344 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:00,344 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:00,344 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,346 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:00,351 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:00,351 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:00,351 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:51:00,352 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:51:00,352 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:51:00,352 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:51:00,352 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:51:00,353 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:51:00,353 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:51:00,353 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:51:00,353 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:51:00,353 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:51:00,353 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:51:00,353 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:51:00,353 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:51:00,353 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:51:00,353 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:51:00,353 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:51:00,353 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:51:00,354 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:51:00,354 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:51:00,354 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:51:00,354 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:51:00,355 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:51:00,355 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:51:03,486 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:51:03,486 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:51:03,486 - wrong_be_tree_count:38	wrong_total:98	 wrong be tree ACC: 0.3877551020408163
2023-07-07 19:51:14,140 - 


2023-07-07 19:51:14,140 - epoch:62,	loss:0.09314388543134555
2023-07-07 19:51:17,736 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:51:17,736 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:51:17,736 - wrong_be_tree_count:35	wrong_total:96	 wrong be tree ACC: 0.3645833333333333
2023-07-07 19:51:27,954 - 


2023-07-07 19:51:27,954 - epoch:63,	loss:0.08827244030544534
2023-07-07 19:51:31,696 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 19:51:31,696 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:51:31,696 - wrong_be_tree_count:36	wrong_total:97	 wrong be tree ACC: 0.3711340206185567
2023-07-07 19:51:38,804 - get train data loader...
2023-07-07 19:51:38,933 - get dev data loader...
2023-07-07 19:51:39,077 - define model...
2023-07-07 19:51:42,053 - 


2023-07-07 19:51:42,054 - epoch:64,	loss:0.08594350359635428
2023-07-07 19:51:43,976 - define optimizer...
2023-07-07 19:51:43,977 - ===========================train setting parameters=========================
2023-07-07 19:51:43,978 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:51:43,978 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:51:43,979 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:51:43,979 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,979 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,979 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,979 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,979 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,980 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,981 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,981 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,982 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,982 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,983 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,983 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,983 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,983 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,984 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,985 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,985 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,985 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,985 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,986 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,986 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,986 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,986 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,987 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,987 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,987 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,987 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,987 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,988 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,988 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,988 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,989 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,990 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,991 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,991 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,991 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,991 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,991 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,992 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,992 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,992 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,992 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,992 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,992 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,993 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,993 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,993 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,993 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,994 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,995 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,995 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,995 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,995 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,995 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,996 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,996 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,996 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,996 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:43,997 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:43,998 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:51:43,998 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:43,998 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:43,998 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:43,999 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:43,999 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:43,999 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:43,999 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:43,999 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:44,000 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,000 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,000 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,000 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,001 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,001 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:44,001 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,002 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:44,003 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:44,003 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:44,003 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,003 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,004 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,004 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:44,004 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:44,004 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:44,005 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:44,005 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:44,005 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:44,005 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,005 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,006 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,006 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,006 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:44,006 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:44,006 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:44,007 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,007 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,008 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,008 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:44,008 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:44,008 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:44,008 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,009 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,009 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,009 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:51:44,009 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:51:44,009 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:51:44,009 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:51:44,010 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:51:44,010 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:51:44,010 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,010 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,010 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,011 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,011 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:51:44,011 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:51:44,011 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:51:44,012 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:51:44,012 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:51:44,012 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:51:44,012 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:51:44,013 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:51:44,013 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:51:44,013 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:51:44,013 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:51:44,013 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:51:44,014 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:51:44,014 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:51:44,014 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:51:44,015 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:51:44,015 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:51:44,015 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:51:44,015 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:51:44,016 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:51:44,016 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:51:44,016 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:51:44,016 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:51:44,016 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:51:44,017 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:51:44,017 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:51:44,017 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:51:44,017 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:51:44,017 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:51:44,018 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 19:51:46,112 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:51:46,112 - right_codes_count:322	total:433	 Code ACC: 0.74364896073903
2023-07-07 19:51:46,112 - wrong_be_tree_count:35	wrong_total:96	 wrong be tree ACC: 0.3645833333333333
2023-07-07 19:51:56,835 - 


2023-07-07 19:51:56,835 - epoch:65,	loss:0.08732218173099682
2023-07-07 19:52:01,976 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:52:01,976 - right_codes_count:323	total:433	 Code ACC: 0.745958429561201
2023-07-07 19:52:01,976 - wrong_be_tree_count:35	wrong_total:96	 wrong be tree ACC: 0.3645833333333333
2023-07-07 19:52:12,368 - 


2023-07-07 19:52:12,369 - epoch:66,	loss:0.08377638814272359
2023-07-07 19:52:15,967 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:52:15,968 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 19:52:15,968 - wrong_be_tree_count:41	wrong_total:100	 wrong be tree ACC: 0.41
2023-07-07 19:52:26,157 - 


2023-07-07 19:52:26,158 - epoch:67,	loss:0.08349973935401067
2023-07-07 19:52:30,632 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:52:30,632 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:52:30,632 - wrong_be_tree_count:35	wrong_total:102	 wrong be tree ACC: 0.3431372549019608
2023-07-07 19:52:41,229 - 


2023-07-07 19:52:41,229 - epoch:68,	loss:0.0828713407390751
2023-07-07 19:52:44,627 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:52:44,627 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:52:44,627 - wrong_be_tree_count:36	wrong_total:100	 wrong be tree ACC: 0.36
2023-07-07 19:52:55,686 - 


2023-07-07 19:52:55,686 - epoch:69,	loss:0.08086115005426109
2023-07-07 19:52:59,479 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:52:59,479 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 19:52:59,479 - wrong_be_tree_count:33	wrong_total:98	 wrong be tree ACC: 0.336734693877551
2023-07-07 19:53:10,355 - 


2023-07-07 19:53:10,356 - epoch:70,	loss:0.08035694636055268
2023-07-07 19:53:13,827 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:53:13,827 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:53:13,827 - wrong_be_tree_count:36	wrong_total:100	 wrong be tree ACC: 0.36
2023-07-07 19:53:24,144 - 


2023-07-07 19:53:24,144 - epoch:71,	loss:0.07946845726110041
2023-07-07 19:53:27,863 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:53:27,863 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:53:27,863 - wrong_be_tree_count:34	wrong_total:101	 wrong be tree ACC: 0.33663366336633666
2023-07-07 19:53:38,225 - 


2023-07-07 19:53:38,225 - epoch:72,	loss:0.07922233350109309
2023-07-07 19:53:41,781 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:53:41,781 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:53:41,781 - wrong_be_tree_count:39	wrong_total:99	 wrong be tree ACC: 0.3939393939393939
2023-07-07 19:53:52,175 - 


2023-07-07 19:53:52,176 - epoch:73,	loss:0.07733319030376151
2023-07-07 19:53:55,771 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:53:55,772 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 19:53:55,772 - wrong_be_tree_count:42	wrong_total:101	 wrong be tree ACC: 0.4158415841584158
2023-07-07 19:54:05,919 - 


2023-07-07 19:54:05,920 - epoch:74,	loss:0.07763060863362625
2023-07-07 19:54:09,287 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:54:09,287 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:54:09,287 - wrong_be_tree_count:41	wrong_total:99	 wrong be tree ACC: 0.41414141414141414
2023-07-07 19:54:19,396 - 


2023-07-07 19:54:19,396 - epoch:75,	loss:0.07559531246079132
2023-07-07 19:54:22,849 - right_count:329	total:433	 Answer ACC: 0.7598152424942263
2023-07-07 19:54:22,850 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:54:22,850 - wrong_be_tree_count:38	wrong_total:104	 wrong be tree ACC: 0.36538461538461536
2023-07-07 19:54:33,448 - 


2023-07-07 19:54:33,449 - epoch:76,	loss:0.07566534320358187
2023-07-07 19:54:38,029 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:54:38,029 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:54:38,029 - wrong_be_tree_count:39	wrong_total:102	 wrong be tree ACC: 0.38235294117647056
2023-07-07 19:54:48,405 - 


2023-07-07 19:54:48,405 - epoch:77,	loss:0.073867976636393
2023-07-07 19:54:52,078 - right_count:331	total:433	 Answer ACC: 0.7644341801385681
2023-07-07 19:54:52,078 - right_codes_count:317	total:433	 Code ACC: 0.7321016166281755
2023-07-07 19:54:52,078 - wrong_be_tree_count:37	wrong_total:102	 wrong be tree ACC: 0.3627450980392157
2023-07-07 19:55:02,330 - 


2023-07-07 19:55:02,330 - epoch:78,	loss:0.07273065089248121
2023-07-07 19:55:06,036 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:55:06,036 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:55:06,036 - wrong_be_tree_count:38	wrong_total:99	 wrong be tree ACC: 0.3838383838383838
2023-07-07 19:55:16,132 - 


2023-07-07 19:55:16,132 - epoch:79,	loss:0.07411862254957668
2023-07-07 19:55:19,621 - right_count:330	total:433	 Answer ACC: 0.7621247113163973
2023-07-07 19:55:19,621 - right_codes_count:316	total:433	 Code ACC: 0.7297921478060047
2023-07-07 19:55:19,621 - wrong_be_tree_count:40	wrong_total:103	 wrong be tree ACC: 0.3883495145631068
2023-07-07 19:55:29,638 - 


2023-07-07 19:55:29,638 - epoch:80,	loss:0.07046729544526897
2023-07-07 19:55:33,114 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:55:33,115 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 19:55:33,115 - wrong_be_tree_count:38	wrong_total:100	 wrong be tree ACC: 0.38
2023-07-07 19:55:43,223 - 


2023-07-07 19:55:43,223 - epoch:81,	loss:0.07211166311753914
2023-07-07 19:55:46,559 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:55:46,559 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:55:46,559 - wrong_be_tree_count:41	wrong_total:98	 wrong be tree ACC: 0.41836734693877553
2023-07-07 19:55:56,803 - 


2023-07-07 19:55:56,804 - epoch:82,	loss:0.07260659273015335
2023-07-07 19:56:00,334 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:56:00,335 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:56:00,335 - wrong_be_tree_count:40	wrong_total:98	 wrong be tree ACC: 0.40816326530612246
2023-07-07 19:56:10,909 - 


2023-07-07 19:56:10,909 - epoch:83,	loss:0.07379983167629689
2023-07-07 19:56:14,367 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:56:14,367 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:56:14,367 - wrong_be_tree_count:37	wrong_total:98	 wrong be tree ACC: 0.37755102040816324
2023-07-07 19:56:24,879 - 


2023-07-07 19:56:24,880 - epoch:84,	loss:0.06828910270996857
2023-07-07 19:56:28,509 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:56:28,509 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:56:28,509 - wrong_be_tree_count:38	wrong_total:100	 wrong be tree ACC: 0.38
2023-07-07 19:56:38,771 - 


2023-07-07 19:56:38,771 - epoch:85,	loss:0.06814510430558585
2023-07-07 19:56:42,213 - right_count:333	total:433	 Answer ACC: 0.76905311778291
2023-07-07 19:56:42,213 - right_codes_count:319	total:433	 Code ACC: 0.7367205542725174
2023-07-07 19:56:42,213 - wrong_be_tree_count:38	wrong_total:100	 wrong be tree ACC: 0.38
2023-07-07 19:56:52,329 - 


2023-07-07 19:56:52,330 - epoch:86,	loss:0.06741621275432408
2023-07-07 19:56:55,971 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:56:55,971 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 19:56:55,971 - wrong_be_tree_count:37	wrong_total:99	 wrong be tree ACC: 0.37373737373737376
2023-07-07 19:57:06,067 - 


2023-07-07 19:57:06,068 - epoch:87,	loss:0.06805274708312936
2023-07-07 19:57:09,455 - right_count:332	total:433	 Answer ACC: 0.766743648960739
2023-07-07 19:57:09,455 - right_codes_count:318	total:433	 Code ACC: 0.7344110854503464
2023-07-07 19:57:09,455 - wrong_be_tree_count:39	wrong_total:101	 wrong be tree ACC: 0.38613861386138615
2023-07-07 19:57:19,722 - 


2023-07-07 19:57:19,723 - epoch:88,	loss:0.06720578420208767
2023-07-07 19:57:23,576 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:57:23,576 - right_codes_count:321	total:433	 Code ACC: 0.7413394919168591
2023-07-07 19:57:23,576 - wrong_be_tree_count:39	wrong_total:98	 wrong be tree ACC: 0.3979591836734694
2023-07-07 19:57:33,818 - 


2023-07-07 19:57:33,818 - epoch:89,	loss:0.06488450121833012
2023-07-07 19:57:37,205 - right_count:338	total:433	 Answer ACC: 0.7806004618937644
2023-07-07 19:57:37,206 - right_codes_count:322	total:433	 Code ACC: 0.74364896073903
2023-07-07 19:57:37,206 - wrong_be_tree_count:39	wrong_total:95	 wrong be tree ACC: 0.4105263157894737
2023-07-07 19:57:37,208 - save best model to ./output/test/best_model
2023-07-07 19:57:52,788 - 


2023-07-07 19:57:52,788 - epoch:90,	loss:0.06638540889252909
2023-07-07 19:57:56,452 - right_count:334	total:433	 Answer ACC: 0.7713625866050808
2023-07-07 19:57:56,452 - right_codes_count:320	total:433	 Code ACC: 0.7390300230946882
2023-07-07 19:57:56,452 - wrong_be_tree_count:37	wrong_total:99	 wrong be tree ACC: 0.37373737373737376
2023-07-07 19:58:06,657 - 


2023-07-07 19:58:06,657 - epoch:91,	loss:0.06421625366783701
2023-07-07 19:58:10,071 - right_count:335	total:433	 Answer ACC: 0.7736720554272517
2023-07-07 19:58:10,071 - right_codes_count:322	total:433	 Code ACC: 0.74364896073903
2023-07-07 19:58:10,071 - wrong_be_tree_count:37	wrong_total:98	 wrong be tree ACC: 0.37755102040816324
2023-07-07 19:58:20,107 - 


2023-07-07 19:58:20,107 - epoch:92,	loss:0.06517635160707869
2023-07-07 19:58:23,493 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:58:23,493 - right_codes_count:323	total:433	 Code ACC: 0.745958429561201
2023-07-07 19:58:23,493 - wrong_be_tree_count:38	wrong_total:96	 wrong be tree ACC: 0.3958333333333333
2023-07-07 19:58:33,933 - 


2023-07-07 19:58:33,933 - epoch:93,	loss:0.0637500777374953
2023-07-07 19:58:37,464 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:58:37,464 - right_codes_count:325	total:433	 Code ACC: 0.7505773672055427
2023-07-07 19:58:37,464 - wrong_be_tree_count:38	wrong_total:96	 wrong be tree ACC: 0.3958333333333333
2023-07-07 19:58:47,645 - 


2023-07-07 19:58:47,645 - epoch:94,	loss:0.06336931425903458
2023-07-07 19:58:50,983 - right_count:337	total:433	 Answer ACC: 0.7782909930715936
2023-07-07 19:58:50,983 - right_codes_count:325	total:433	 Code ACC: 0.7505773672055427
2023-07-07 19:58:50,983 - wrong_be_tree_count:37	wrong_total:96	 wrong be tree ACC: 0.3854166666666667
2023-07-07 19:59:01,553 - 


2023-07-07 19:59:01,553 - epoch:95,	loss:0.06337087557767518
2023-07-07 19:59:04,976 - right_count:336	total:433	 Answer ACC: 0.7759815242494227
2023-07-07 19:59:04,976 - right_codes_count:323	total:433	 Code ACC: 0.745958429561201
2023-07-07 19:59:04,976 - wrong_be_tree_count:35	wrong_total:97	 wrong be tree ACC: 0.36082474226804123
2023-07-07 19:59:15,826 - 


2023-07-07 19:59:15,826 - epoch:96,	loss:0.0653396401903592
2023-07-07 19:59:39,240 - get train data loader...
2023-07-07 19:59:39,523 - get dev data loader...
2023-07-07 19:59:39,662 - define model...
2023-07-07 19:59:43,896 - define optimizer...
2023-07-07 19:59:43,899 - ===========================train setting parameters=========================
2023-07-07 19:59:43,899 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 19:59:43,899 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 19:59:43,901 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 19:59:43,901 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,901 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,901 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,901 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,901 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,901 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,902 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,902 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,902 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,903 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,903 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,903 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,903 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,904 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,904 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,904 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,904 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,905 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,905 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,905 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,906 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,906 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,906 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,906 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,907 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,908 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,908 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,909 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,909 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,909 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,909 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,910 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,911 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,912 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,912 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,912 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,917 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,917 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,917 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,918 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,918 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,918 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,919 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,919 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,919 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,920 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,920 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,921 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,922 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,922 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,923 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,923 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,923 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,924 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,924 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,925 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,925 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,925 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,925 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,925 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,926 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,926 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,926 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,926 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,927 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,927 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,928 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,928 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,929 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,929 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,930 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,930 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,931 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,932 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,932 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,933 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,934 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,934 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,934 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,936 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,936 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,937 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,937 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,937 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,938 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,938 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,939 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,940 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,940 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,941 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,941 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,941 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,942 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,942 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,942 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,943 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,943 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,943 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,945 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,945 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,945 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,946 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,946 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,947 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,948 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,948 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,948 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,948 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,949 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,949 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,949 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,950 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,950 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,950 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,950 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,951 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,951 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,951 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,952 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,952 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,952 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,953 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,953 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,953 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,954 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,954 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,954 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,955 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,955 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,955 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,955 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,956 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,957 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,957 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,958 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,958 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,959 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,959 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,960 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,961 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,961 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,961 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,962 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,962 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,963 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,963 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 19:59:43,964 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 19:59:43,965 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 19:59:43,965 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 19:59:43,966 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 19:59:43,966 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 19:59:43,967 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,967 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,967 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,967 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,968 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 19:59:43,968 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 19:59:43,969 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 19:59:43,969 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 19:59:43,969 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 19:59:43,970 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 19:59:43,970 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 19:59:43,970 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 19:59:43,970 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 19:59:43,970 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 19:59:43,970 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 19:59:43,971 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 19:59:43,971 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 19:59:43,971 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 19:59:43,971 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 19:59:43,971 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 19:59:43,971 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 19:59:43,972 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 19:59:43,972 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 19:59:43,972 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 19:59:43,972 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 19:59:43,972 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 19:59:43,973 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 19:59:43,973 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 19:59:43,974 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 19:59:43,974 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 19:59:43,974 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 19:59:43,974 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 19:59:43,975 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 19:59:43,975 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 20:06:23,402 - get train data loader...
2023-07-07 20:06:23,543 - get dev data loader...
2023-07-07 20:06:23,629 - define model...
2023-07-07 20:06:28,399 - define optimizer...
2023-07-07 20:06:28,400 - ===========================train setting parameters=========================
2023-07-07 20:06:28,400 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 20:06:28,401 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 20:06:28,401 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 20:06:28,401 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,402 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,403 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,403 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,403 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,403 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,403 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,404 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,405 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,405 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,405 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,406 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,406 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,407 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,407 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,407 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,407 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,408 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,408 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,408 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,409 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,409 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,409 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,409 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,410 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,410 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,410 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,411 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,411 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,412 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,412 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,412 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,412 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,413 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,413 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,413 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,413 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,414 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,414 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,414 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,415 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,415 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,415 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,415 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,415 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,416 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,416 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,416 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,417 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,417 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,417 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,417 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,417 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,417 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,417 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,418 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,419 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,419 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,419 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,420 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,420 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,420 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,420 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,420 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,421 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,421 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,421 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,421 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,421 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,422 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,422 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,422 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,422 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,424 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,424 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,424 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,424 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,425 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,425 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,425 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,425 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,426 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,426 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,426 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,426 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,426 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,427 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,427 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,427 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,428 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,428 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,428 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,428 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,429 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,429 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,429 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,429 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,430 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,430 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,430 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,430 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,431 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,431 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,431 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,431 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,431 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,436 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,437 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,437 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,437 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,437 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,438 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,439 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,439 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,439 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,440 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,440 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,440 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,440 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,440 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,441 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,441 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,441 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,441 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,441 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:06:28,441 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 20:06:28,442 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:06:28,442 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 20:06:28,442 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:06:28,442 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 20:06:28,442 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:06:28,443 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:06:28,444 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 20:06:28,444 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:06:28,444 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:06:28,444 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 20:06:28,444 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 20:06:28,445 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 20:06:28,445 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 20:06:28,445 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 20:06:28,445 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 20:06:28,445 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 20:06:28,446 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 20:06:28,446 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 20:06:28,446 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 20:06:28,446 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 20:06:28,446 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 20:06:28,446 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 20:06:28,447 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 20:06:28,447 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 20:06:28,447 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 20:06:28,447 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 20:06:28,447 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 20:06:28,447 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 20:06:28,451 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 20:06:28,451 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 20:06:28,451 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 20:06:28,451 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 20:06:28,451 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 20:07:24,244 - get train data loader...
2023-07-07 20:07:24,371 - get dev data loader...
2023-07-07 20:07:24,464 - define model...
2023-07-07 20:07:28,442 - define optimizer...
2023-07-07 20:07:28,443 - ===========================train setting parameters=========================
2023-07-07 20:07:28,443 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 20:07:28,443 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 20:07:28,443 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 20:07:28,443 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,444 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,444 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,445 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,446 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,447 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,447 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,447 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,447 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,447 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,447 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,447 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,447 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,448 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,449 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,449 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,449 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,449 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,449 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,449 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,449 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,449 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,450 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,451 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,451 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,451 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,451 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,451 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,451 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,451 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,451 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,452 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,453 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,453 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,453 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,454 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,454 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,454 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,454 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,454 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,455 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,455 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,455 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,455 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,456 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,457 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,458 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,458 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,458 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,458 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,458 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,458 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,458 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,459 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,460 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,460 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,460 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,460 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,460 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,460 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,460 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,460 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,461 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,462 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,462 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,462 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 20:07:28,463 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,464 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,464 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,464 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,464 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:07:28,464 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:07:28,465 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:07:28,465 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 20:07:28,465 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:07:28,465 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:07:28,465 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 20:07:28,465 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 20:07:28,465 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 20:07:28,465 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 20:07:28,466 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 20:07:28,466 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 20:07:28,466 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 20:07:28,466 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 20:07:28,466 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 20:07:28,466 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 20:07:28,466 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 20:07:28,466 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 20:07:28,466 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 20:07:28,466 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 20:07:28,466 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 20:07:28,467 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 20:07:28,467 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 20:07:28,467 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 20:07:28,467 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 20:07:28,467 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 20:07:28,467 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 20:07:28,467 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 20:07:28,467 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 20:07:28,467 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
2023-07-07 20:07:59,361 - get train data loader...
2023-07-07 20:07:59,743 - get dev data loader...
2023-07-07 20:07:59,905 - define model...
2023-07-07 20:08:04,303 - define optimizer...
2023-07-07 20:08:04,306 - ===========================train setting parameters=========================
2023-07-07 20:08:04,306 - bert.embeddings.word_embeddings.weight-torch.Size([30522, 768])
2023-07-07 20:08:04,307 - bert.embeddings.position_embeddings.weight-torch.Size([512, 768])
2023-07-07 20:08:04,307 - bert.embeddings.token_type_embeddings.weight-torch.Size([2, 768])
2023-07-07 20:08:04,308 - bert.embeddings.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,308 - bert.embeddings.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,310 - bert.encoder.layer.0.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,310 - bert.encoder.layer.0.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,310 - bert.encoder.layer.0.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,310 - bert.encoder.layer.0.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,311 - bert.encoder.layer.0.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,311 - bert.encoder.layer.0.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,311 - bert.encoder.layer.0.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,311 - bert.encoder.layer.0.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,312 - bert.encoder.layer.0.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,312 - bert.encoder.layer.0.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,313 - bert.encoder.layer.0.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,313 - bert.encoder.layer.0.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,314 - bert.encoder.layer.0.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,314 - bert.encoder.layer.0.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,315 - bert.encoder.layer.0.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,315 - bert.encoder.layer.0.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,316 - bert.encoder.layer.1.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,316 - bert.encoder.layer.1.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,317 - bert.encoder.layer.1.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,317 - bert.encoder.layer.1.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,318 - bert.encoder.layer.1.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,318 - bert.encoder.layer.1.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,319 - bert.encoder.layer.1.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,319 - bert.encoder.layer.1.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,319 - bert.encoder.layer.1.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,320 - bert.encoder.layer.1.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,320 - bert.encoder.layer.1.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,320 - bert.encoder.layer.1.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,321 - bert.encoder.layer.1.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,321 - bert.encoder.layer.1.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,322 - bert.encoder.layer.1.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,322 - bert.encoder.layer.1.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,323 - bert.encoder.layer.2.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,323 - bert.encoder.layer.2.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,323 - bert.encoder.layer.2.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,323 - bert.encoder.layer.2.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,323 - bert.encoder.layer.2.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,324 - bert.encoder.layer.2.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,324 - bert.encoder.layer.2.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,324 - bert.encoder.layer.2.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,325 - bert.encoder.layer.2.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,325 - bert.encoder.layer.2.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,325 - bert.encoder.layer.2.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,326 - bert.encoder.layer.2.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,326 - bert.encoder.layer.2.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,327 - bert.encoder.layer.2.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,327 - bert.encoder.layer.2.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,327 - bert.encoder.layer.2.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,327 - bert.encoder.layer.3.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,327 - bert.encoder.layer.3.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,328 - bert.encoder.layer.3.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,328 - bert.encoder.layer.3.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,328 - bert.encoder.layer.3.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,329 - bert.encoder.layer.3.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,329 - bert.encoder.layer.3.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,330 - bert.encoder.layer.3.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,330 - bert.encoder.layer.3.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,331 - bert.encoder.layer.3.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,332 - bert.encoder.layer.3.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,333 - bert.encoder.layer.3.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,333 - bert.encoder.layer.3.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,334 - bert.encoder.layer.3.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,334 - bert.encoder.layer.3.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,335 - bert.encoder.layer.3.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,335 - bert.encoder.layer.4.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,335 - bert.encoder.layer.4.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,335 - bert.encoder.layer.4.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,335 - bert.encoder.layer.4.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,336 - bert.encoder.layer.4.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,336 - bert.encoder.layer.4.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,336 - bert.encoder.layer.4.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,336 - bert.encoder.layer.4.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,337 - bert.encoder.layer.4.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,337 - bert.encoder.layer.4.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,337 - bert.encoder.layer.4.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,338 - bert.encoder.layer.4.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,338 - bert.encoder.layer.4.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,338 - bert.encoder.layer.4.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,338 - bert.encoder.layer.4.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,338 - bert.encoder.layer.4.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,339 - bert.encoder.layer.5.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,340 - bert.encoder.layer.5.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,340 - bert.encoder.layer.5.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,340 - bert.encoder.layer.5.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,340 - bert.encoder.layer.5.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,340 - bert.encoder.layer.5.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,341 - bert.encoder.layer.5.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,341 - bert.encoder.layer.5.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,341 - bert.encoder.layer.5.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,341 - bert.encoder.layer.5.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,341 - bert.encoder.layer.5.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,342 - bert.encoder.layer.6.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,342 - bert.encoder.layer.6.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,342 - bert.encoder.layer.6.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,342 - bert.encoder.layer.6.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,342 - bert.encoder.layer.6.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,343 - bert.encoder.layer.6.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,344 - bert.encoder.layer.6.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,344 - bert.encoder.layer.6.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,344 - bert.encoder.layer.6.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,345 - bert.encoder.layer.6.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,345 - bert.encoder.layer.6.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,345 - bert.encoder.layer.7.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,346 - bert.encoder.layer.7.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,347 - bert.encoder.layer.7.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,347 - bert.encoder.layer.7.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,347 - bert.encoder.layer.7.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,347 - bert.encoder.layer.7.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,348 - bert.encoder.layer.7.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,348 - bert.encoder.layer.7.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,349 - bert.encoder.layer.7.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,349 - bert.encoder.layer.7.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,350 - bert.encoder.layer.7.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,350 - bert.encoder.layer.8.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,351 - bert.encoder.layer.8.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,351 - bert.encoder.layer.8.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,351 - bert.encoder.layer.8.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,352 - bert.encoder.layer.8.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,352 - bert.encoder.layer.8.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,352 - bert.encoder.layer.8.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,352 - bert.encoder.layer.8.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,352 - bert.encoder.layer.8.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,353 - bert.encoder.layer.8.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,353 - bert.encoder.layer.8.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,353 - bert.encoder.layer.8.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,353 - bert.encoder.layer.8.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,353 - bert.encoder.layer.8.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,354 - bert.encoder.layer.8.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,354 - bert.encoder.layer.8.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,354 - bert.encoder.layer.9.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,355 - bert.encoder.layer.9.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,355 - bert.encoder.layer.9.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,355 - bert.encoder.layer.9.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,356 - bert.encoder.layer.9.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,357 - bert.encoder.layer.9.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,357 - bert.encoder.layer.10.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,358 - bert.encoder.layer.10.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,358 - bert.encoder.layer.10.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,358 - bert.encoder.layer.10.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,358 - bert.encoder.layer.10.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,359 - bert.encoder.layer.10.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,359 - bert.encoder.layer.10.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,359 - bert.encoder.layer.10.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,360 - bert.encoder.layer.10.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,361 - bert.encoder.layer.10.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,361 - bert.encoder.layer.10.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,361 - bert.encoder.layer.11.attention.self.query.weight-torch.Size([768, 768])
2023-07-07 20:08:04,361 - bert.encoder.layer.11.attention.self.query.bias-torch.Size([768])
2023-07-07 20:08:04,361 - bert.encoder.layer.11.attention.self.key.weight-torch.Size([768, 768])
2023-07-07 20:08:04,362 - bert.encoder.layer.11.attention.self.key.bias-torch.Size([768])
2023-07-07 20:08:04,362 - bert.encoder.layer.11.attention.self.value.weight-torch.Size([768, 768])
2023-07-07 20:08:04,362 - bert.encoder.layer.11.attention.self.value.bias-torch.Size([768])
2023-07-07 20:08:04,363 - bert.encoder.layer.11.attention.output.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,363 - bert.encoder.layer.11.attention.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,363 - bert.encoder.layer.11.attention.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,363 - bert.encoder.layer.11.attention.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,363 - bert.encoder.layer.11.intermediate.dense.weight-torch.Size([3072, 768])
2023-07-07 20:08:04,364 - bert.encoder.layer.11.intermediate.dense.bias-torch.Size([3072])
2023-07-07 20:08:04,364 - bert.encoder.layer.11.output.dense.weight-torch.Size([768, 3072])
2023-07-07 20:08:04,364 - bert.encoder.layer.11.output.dense.bias-torch.Size([768])
2023-07-07 20:08:04,364 - bert.encoder.layer.11.output.LayerNorm.weight-torch.Size([768])
2023-07-07 20:08:04,364 - bert.encoder.layer.11.output.LayerNorm.bias-torch.Size([768])
2023-07-07 20:08:04,364 - bert.pooler.dense.weight-torch.Size([768, 768])
2023-07-07 20:08:04,365 - bert.pooler.dense.bias-torch.Size([768])
2023-07-07 20:08:04,365 - fc.layer1.0.weight-torch.Size([2048, 1536])
2023-07-07 20:08:04,366 - fc.layer1.0.bias-torch.Size([2048])
2023-07-07 20:08:04,366 - fc.layer2.0.weight-torch.Size([1024, 2048])
2023-07-07 20:08:04,366 - fc.layer2.0.bias-torch.Size([1024])
2023-07-07 20:08:04,366 - fc.layer3.0.weight-torch.Size([28, 1024])
2023-07-07 20:08:04,367 - fc.layer3.0.bias-torch.Size([28])
2023-07-07 20:08:04,367 - code_emb.layer1.0.weight-torch.Size([384, 28])
2023-07-07 20:08:04,367 - code_emb.layer1.0.bias-torch.Size([384])
2023-07-07 20:08:04,367 - code_emb.layer2.0.weight-torch.Size([768, 384])
2023-07-07 20:08:04,367 - code_emb.layer2.0.bias-torch.Size([768])
2023-07-07 20:08:04,367 - code_emb.layer3.0.weight-torch.Size([1536, 768])
2023-07-07 20:08:04,368 - code_emb.layer3.0.bias-torch.Size([1536])
2023-07-07 20:08:04,368 - code_check.layer1.0.weight-torch.Size([768, 1536])
2023-07-07 20:08:04,368 - code_check.layer1.0.bias-torch.Size([768])
2023-07-07 20:08:04,368 - code_check.layer2.0.weight-torch.Size([384, 768])
2023-07-07 20:08:04,369 - code_check.layer2.0.bias-torch.Size([384])
2023-07-07 20:08:04,369 - code_check.layer3.0.weight-torch.Size([2, 384])
2023-07-07 20:08:04,370 - code_check.layer3.0.bias-torch.Size([2])
2023-07-07 20:08:04,370 - dec_self_attn.W_Q.weight-torch.Size([1024, 1536])
2023-07-07 20:08:04,371 - dec_self_attn.W_K.weight-torch.Size([1024, 1536])
2023-07-07 20:08:04,371 - dec_self_attn.W_V.weight-torch.Size([1024, 1536])
2023-07-07 20:08:04,372 - dec_self_attn.fc.weight-torch.Size([1536, 1024])
